---
pdf_document: default
output: pdf_document
author: "Antonio Vivone"
title: "Statistica e analisi dei dati"
subtitle: "Progetto di corso - analisi di un dataset"
fontsize: 12pt
---

# Introduzione
Mai come al giorno d'oggi il tema immigrazione risulta essere attuale, trattato spesso dai media e discusso dai politici. 
L'immigrazione è anche il tema alla base di questo progetto il quale scopo è quello di analizzare i dati forniti da un'indagine effettuata dall'ISTAT. Il titolo della tavola utilizzata è *Cittadini non comunitari regolarmente presenti per motivo della presenza* e, come si evince da questo, il progetto si focalizza sul motivo della presenza di questi cittadini. 

Per definizione, i cittadini comunitari sono tutti coloro che risultano essere in possesso della cittadinanza di uno degli stati membri della Comunità Europea. È bene ricordare che gli stati facenti parte dell'Unione Europea ad oggi sono: Austria, Belgio, Bulgaria, Cipro, Croazia, Danimarca, Estonia, Finlandia, Francia, Germania, Grecia, Irlanda, Italia, Lettona, Lituania, Lussemburgo, Malta, Paesi Bassi, Polonia, Portogallo, Regno Unito, Repubblica Ceca, Romania, Slovacchia, Slovenia, Spagna, Svezia, Ungheria.
Lo studio è stato effettuato su cittadini non comunitari, quindi aventi cittadinanza di altri paesi.

I dati risultano essere aggiornati al 1 gennaio 2018. 
La tabella utilizzata è composta da 20 righe rappresentanti le regioni italiane e 5 colonne rappresentanti le maggiori motivazioni, in particolare: lavoro, famiglia, studio, asilo e altro. I dati sono sottoforma di percentuale.
Utilizzando metodi statistici, il progetto verterà sul raggiungere diversi obiettivi:

* Individuare i legami fra le diverse modalità assunte.
* Individuare valori *outlier*, ovvero anomali perchè fuori dal range
* Definire cluster per raggruppare regioni ritenute simili

Il linguaggio di programmazione utilizzato all'interno del progetto per eseguire l'analisi dei dati è R.

La prima cosa da fare è costruire la tabella all'interno del workspace di R. Iniziamo quindi a definire per ogni modalità un vettore e inserire i rispettivi valori della colonna.

```{r}
lavoro <- c(28.3, 26.1, 30.9, 38.8, 22.1, 35.2, 23.5, 31.4, 39.3, 30.5,
            31.4, 34.1, 28.0, 10.9, 41.6, 21.6, 21.6, 23.6, 28.0, 28.2)
famiglia <- c(46.6, 53.2, 44.8, 47.3, 50.9, 48.6, 44.5, 50.7, 39.8, 48.6, 
              45.0, 37.1, 45.7, 18.6, 29.1, 29.3, 25.9, 29.2, 32.8, 34.4)
studio <- c(4.8, 1.9, 2.8, 3.1, 2.6, 1.5, 2.8, 3.2, 3.2, 5.1, 3.1, 4.7, 
            2.3, 0.7, 1.0, 1.6, 1.1, 1.8, 0.7, 2.1)
asilo <- c(18.1, 16.6, 17.7, 9.4, 22.4, 13.2, 26.6, 12.7, 14.5, 12.5, 18.1,
           14.3, 20.3, 67.1, 23.5, 43.0, 46.0, 42.8, 32.6, 31.0)
altro <- c(2.16, 2.20, 3.86, 1.57, 1.90, 1.54, 2.58, 2.04, 3.23, 3.29, 2.46,
           9.79, 3.70, 2.75, 4.83, 4.40, 5.46, 2.55, 5.87, 4.33)
```

Utilizzando il comando *cbind* è possibile creare una matrice a partire da vettori della stessa lunghezza. Inoltre i dati verranno inseriti per colonne (per questo la c davanti bind). Per inserire i dati per riga, il comando da utilizzare è *rbind*

```{r}
tabella <- cbind(lavoro, famiglia, studio, asilo, altro)
```

A questo punto utilizziamo la funzione *rownames* a cui diamo in input la matrice creata in precedenza. Questa funzione ci permette di assegnare ad ogni riga un nome, che in questo caso è quello delle regioni italiane. Fatto questo stampiamo la matrice appena creata semplicemente scrivendone il nome.


```{r}
rownames(tabella) <- c("Piemonte", "Valle d'Aos", "Liguria",
                       "Lombardia", "T-A Adige","Veneto", 
                       "F-V Giulia", "Em-Romag", "Toscana", 
                       "Umbria", "Marche", "Lazio", "Abruzzo", 
                       "Molise", "Campania", "Puglia", 
                       "Basilicata", "Calabria", "Sicilia", "Sardegna")
tabella
```
\newpage

# Distribuzioni di frequenza semplice

Nella statistica descrittiva è possibile avere tre tipologie di variabili:

1. **Variabili qualitative**, ovvero variabili il cui valore non è numerico. Ad esempio, il colore dei capelli

2. **Variabili quantitative**, variabile il cui valore è di tipo numerico. Ad esempio, l'altezza.

3. **Variabili ordinabili**, in grado di essere ordinate. I valori possono essere di tipo numerico o testuale.

I dati a nostra dispozione all'interno della tabella sono di tipo numerico e ricadono quindi nella seconda tipologia descritta.
Utilizziamo le *distribuzioni di frequenza* per analizzare la distribuzione dei dati, andando a sintetizzare ciò che essi rappresentano.

Siccome molto spesso è preferibile raggruppare le informazioni in classi quando si hanno variabili quantitative a disposizione, sono state definite 5 classi:

```{r}
classi <- c(0, 15, 20, 30, 50, 70)
```

Definite le classi andiamo ad utlizzare un altro comando, *cut()*, a cui diamo in input il vettore dei dati da classificare e il vettore delle classi andando così a inserire ogni valore nell'intervallo corrispondente.

## Frequenza assoluta

Per costruire una distribuzione di frequenza in R utilizziamo la funzione *table()* che ci aiuta a calcola le frequenze assolute dei dati a disposizione.

```{r}
table(cut(lavoro, classi))
```

Da questa suddivisione, possiamo notare due aspetti interessanti: 

* in ben nove regioni almeno 1/3 dei cittadini non comunitari si trova lì per motivi di lavoro.

* solo in una regione, per la precisione nel Molise, il tasso dei cittadini non comunitari che giustifica la propria presenza per questioni di lavoro è estremamente basso rispetto alle altre regioni ed è del 10.9%.

Applichiamo la stessa funzione anche agli altri vettori rappresentanti le restanti modalità ed effettuiamo un'analisi.


```{r}
table(cut(famiglia, classi))
```

Guardando questi dati notiamo che:

* almeno il 50% dei cittadini non comunitari residenti in tre regioni giustifica la propria presenza per questioni legate alla famiglia. Queste regioni sono Valle d'Aosta, Trentino-Alto Adige e Emilia-Romagna.

* solo in una regione il tasso è inferiore del 20%. Riferendoci alla tabella, scopriamo che è sempre il Molise con un tasso del 18.6%.


```{r}
table(cut(studio, classi))
```

Notiamo:

* lo studio è motivo di presenza di una percentuale molto bassa di cittadini non comunitari. Di fatti, guardando alla tabella, è possibile notare che le percentuali sono quasi tutte inferiori al 5%.


```{r}
table(cut(asilo, classi))
```

Notiamo:

* in una sola regione il tasso dei cittadini non comunitari che si trovano lì per asilo è superiore al 50%. Questa regione è il Molise con una percentuale del 67.1%

* in ben sei regioni, la presenza di cittadini non comunitari per asilo è inferiore al 15%, in particolare in Lombardia, Veneto, Emilia-Romagna, Toscana, Umbria e Lazio.

```{r}
table(cut(altro, classi))
```

Notiamo:

* la percentuale di cittadini non comunitari la cui presenza è dovuta a motivi non ben specificati è in tutte le regioni al di sotto del 15%. In particolare, guardando la tabella i valori non superano neanche il 10%.

## Frequenze relative

Per calcolare la distribuzione delle **frequenze relative** basta dividere l'output della funzione *table()* per la lunghezza del vettore considerato, ottenuta con la funzione *length()*

```{r}
table(cut(lavoro, classi))/length(lavoro)
```

Notiamo:

* solo il 5% delle regioni italiane ospita cittadini non comunitari la cui presenza è dovuta al lavoro per un tasso inferiore al 15%
* il restante 95% delle regioni è abitato da cittadini non comunitari presenti a causa del lavoro per un tasso superiore al 20%

Com'è possibile vedere, i risultati forniti dall'analisi delle frequenze relative corrispondono a quelli delle frequenze assolute.
Completiamo calcolando le frequenze relative per le altre modalità.

```{r}
table(cut(famiglia, classi))/length(famiglia)
```

```{r}
table(cut(studio, classi))/length(studio)
```

```{r}
table(cut(asilo, classi))/length(asilo)
```

```{r}
table(cut(altro, classi))/length(altro)
```

## Frequenze assolute cumulate

In R è possibile cumulare le varie frequenze sfruttando la funzione *cumsum()*.
Vediamo di seguito i valori calcolati

```{r}
cumsum(table(cut(lavoro, classi)))
```

```{r}
cumsum(table(cut(famiglia, classi)))
```

```{r}
cumsum(table(cut(studio, classi)))
```

```{r}
cumsum(table(cut(asilo, classi)))
```

```{r}
cumsum(table(cut(altro, classi)))
```

## Frequenze relative cumulate

```{r}
cumsum(table(cut(lavoro, classi))/length(lavoro))

```

```{r}
cumsum(table(cut(famiglia, classi))/length(famiglia))

```

```{r}
cumsum(table(cut(studio, classi))/length(studio))

```

```{r}
cumsum(table(cut(asilo, classi))/length(asilo))

```

```{r}
cumsum(table(cut(altro, classi))/length(altro))

```

\newpage

# Le rappresentazioni grafiche

Le rappresentazioni grafiche ci permettono di confrontare e analizzare i dati contenuti nella tabella iniziale in maniera decisamente più veloce.
Esistono moltissime tipologie di grafici ma non sempre sono tutti adatti a raggiungere lo scopo o a visualizzare in maniera adatta i dati che si vogliono rappresentare.
La creazione di grafici e la rappresentazione di dati sono attività che ricadono nella data visualization.

Per i dati a nostra disposizione, si è scelto di utilizzare un grafico a barre in modo da rendere chiara la differenza e facilitare i confronto fra le varie regioni.
In R è possibile creare un grafico a barre attraverso il comando *barplot()*.
All'interno del comando *barplot* sono state specificate alcune opzioni, ovvero:

* *col* ci permette di colorare le barre del grafico

* *ylim* per indicare il punto dove far iniziare e far finire l'asse delle ordinate

* *las* per stampare le label dell'asse delle ascisse in verticale

\newpage

In questo primo *barplot* sull'asse delle ascisse sono state inserite le venti regioni italiane, mentre sull'asse delle ordinate i valori della tabella corrispondenti alla colonna "lavoro".

```{r}
barplot(tabella[,1], col=1:20, ylim = c(0, 50), las=2, 
        main = "Cittadini non comunitari 
        residenti per motivi di lavoro")
```

Come visto in precedenza con l'analisi delle frequenze relative, anche qui notiamo che:

* il Molise è la regione con il tasso più basso di cittadini non comunitari la cui presenza è dovuta a ragioni di lavoro.

* la Campania, la Toscana e la Lombardia sono le tre regioni con il tasso più alto.

\newpage

Passiamo adesso all'analisi della modalità "famiglia"

```{r}
barplot(tabella[,2], col=1:20, ylim = c(0,60), las=2, 
        main = "Cittadini non comunitari 
        residenti per motivi di famiglia")
```

Possiamo anche qui vedere che, per quanto riguarda i motivi di famiglia:

* il Molise ha il tasso più basso fra tutte le regioni

* il tasso è più alto del 50% in Valle d'Aosta, Emilia-Romagna e Trentino-Alto Adige

\newpage
Per quanto riguarda lo studio invece:

```{r}
barplot(tabella[,3], col=1:20, ylim = c(0,6), las=2, 
        main = "Cittadini non comunitari 
        residenti per motivi di studio")
```

Quello che si nota è che in generale i cittadini non comunitari non vengono in Italia per studiare in quanto i tassi
risultano estremamente bassi in tutte le regioni con i più bassi in Molise e Sicilia.

\newpage

La situazione di asilo politico è la seguente:

```{r}
barplot(tabella[,4], col=1:20, ylim = c(0,70), las=2, 
        main = "Cittadini non comunitari 
        residenti per asilo")
```


Fra le osservazioni interessanti di questa situzione abbiamo:

* il Molise risulta avere il tasso più alto di cittadini non comunitari rifugiati.

* la Lombardia, insieme a qualche altra regione del nord Italia, presenta il tasso più basso

\newpage

Per quanto riguarda le motivazioni non specificate:

```{r}
barplot(tabella[,5], col=1:20, ylim = c(0,10), las=2, 
        main = "Cittadini non comunitari 
        residenti per altri motivi")
```
È possibile notare che i dati raccolti per motivi non ben specificati sono decisamente bassi.
Si differenzia solo la situazione del Lazio, che presente un tasso decisamente elevato rispetto alle altre.

\newpage

La tabella fornita dall'ISTAT comprende anche una riga riguardante tutta l'Italia. Procediamo quindi alla creazione di un grafico a barre dove sull'asse
orizzontale disponiamo le modalità che possono essere assunte dal campione e sull'asse verticale il valore della corrispondente modalità
Visto che utilizzeremo questi dati solo per la creazione di alcuni grafici, non aggiungiamo la riga alla matrice precedente ma ne creiamo una nuova. 

```{r}
italia <- rbind(c(33.6, 42.6, 3.0, 17.1, 3.63))
colnames(italia) <- c("Lavoro", "Famiglia", "Studio", "Asilo", "Altro") 
barplot(italia[1,], col = 1:5)
```

Come detto prima, R fornisce anche la possibilità di utilizzare altre tipi di diagrammi, come quelli a torta, che suddividono un cerchio in diversi settori tanti quante sono le modalità che può assumere il campione.
Per creare un diagramma a torta in R è possibile utilizzare la funzione *pie()* 
```{r}
pie(italia[1,], col=1:5)
```

È possibile applicare diversi tipi di tratteggio al diagramma a torta, utilizzando linee al posto della tinta unita. Aggiungengo le opzione
*density* e *angle* è possibile impostare la densità delle linee del tratteggio e l'angolo che esse devono assumere.

```{r}
pie(italia[1,], density = 18, angle= 15+10*(1:5), col = 1:5)
```

Queste sono solo due delle diverse tipologie di grafico messe a disposizione da R. È possibile notare che la percezione della distribuzione dataci dal grafico a barre è la stessa di quella del grafico a torta, cambia solo la modalità di visualizzazione dei dati. 

**Diagramma di Pareto**

Il diagramma di Pareto è un grafico che rappresenta l'importanza delle differenze causate da un certo fenomeno. Contiene al suo interno un diagramma a barre verticali con le modalità organizzate in maniera decrescente rispetto alla loro frequenza relativa. Le frequenze relative sono rappresentate sottoforma cumulata come una curva. Il suo scopo è quello di aiutare a capire quali sono i maggiori fattori che hanno influenza su un dato fenomeno.

Prendiamo come modalità in analisi l'asilo politico. Con l'opzione *drop=FALSE* diciamo ad R di includere anche le label della righe.

```{r}
tabella[,4,drop=FALSE]
```

La prima cosa che facciamo è ordinare in maniera decrescente in base alla modalità *asilo* che è quella che vogliamo analizzare.

```{r}
ordinata <- tabella[order(asilo, decreasing=TRUE),4 , drop = FALSE]
ordinata
```

```{r}
asilo_freqRel <- ordinata[,1]/sum(asilo)
asilo_freqRelCum <- cumsum(asilo_freqRel)
asilo_freqRelCum
```

```{r}
pareto <- barplot(asilo_freqRel, ylim = c(0,1.1), col=1:20, las=2)
lines(pareto, asilo_freqRelCum, type = "b", pch=16)
text(pareto-0.01, asilo_freqRelCum+0.03,
     paste(format(asilo_freqRelCum *100, digits=2), "%"))
```

Quello che possiamo notare dal diagramma di Pareto è che circa il 70% dei cittadini non comunitari presenti in Italia per motivi di asilo politico risiede nelle prime dieci regioni sopra elencate. Notasi anche che in queste dieci regioni figura tutto il Sud Italia, composto da ben 8 regioni.

Consideriamo sempre il vettore quantitativo *lavoro*. Attraverso l'utilizzo della funzione *plot()* è possibile visualizzare su un piano cartesiano il valore assunto da ogni regione italiana. Nella seguente riga di codice sono state utilizzate due opzioni, *xlab* e *ylab*, che ci permettono di impostare manualmente il valore dell'etichetta degli assi.

```{r}
plot(lavoro, col="blue", ylim = c(8,50), xlab = "Regioni", ylab="Tasso per lavoro")
```

È possibile connettere i punti del grafico con delle linee utilizzando l'apposito comando *lines()* 

```{r}
plot(lavoro, col="blue", ylim = c(8,50), xlab = "Regioni", ylab="Tasso per lavoro")
lines(lavoro, col="red")
```

La creazione di plot non ci permette di dedurre nuove informazioni rispetto a quello che sappiamo già.

Possiamo anche creare un unico grafico che ci permetta di confrontare i valori assunti dalle diverse modalità. 
Associamo ad ogni modalità un simbolo diverso:

* $*$ per indicare i valori di lavoro
* $+$ per indicare i valori di famiglia
* $x$ per indicare i valori di studio
* $o$ per indicare i valori di asilo
* $-$ per indicare i valori di altro

Useremo la funzione *points()* per aggiungere altri punti al grafico già esistente. Fra i parametri utilizzati compare *pch* che ci permette di indicare quale simbolo utilizzare per rappresentare i valori. La funzione *legend()* serve a creare una legenda all'interno del grafico disegnato. Il parametro *ncol* utilizzato all'interno della funzione *legend()*
ci permette di indicare il numero di colonne da utilizzare.

```{r}
plot(lavoro, pch="*", ylim = c(0, 90), col="blue", 
     ylab="Tasso", xlab="Indici regioni")
points(famiglia, pch="+", col="red")
points(studio, pch="x", col="green")
points(asilo, pch="o", col="magenta")
points(altro, pch="-", col="black")

legend(1, 90, c("lavoro", "famiglia", "studio", "asilo", "altro"), 
      pch=c("*","+", "x", "o", "-"),
      col = c("blue", "red", "green", "magenta", "black"), 
      ncol = 3)

```


### Istogrammi

Gli istogrammi sono una particolare rappresentazione grafica di una distribuzione di frequenza in classi e vengono utilizzati per le variabili *quantitative*
Questa tipologia di rappresentazione grafica viene ottenuta mediante rettangoli adiacenti aventi per basi segmenti i cui estremi corrispondono agli estremi delle classi. Fissate le basi, le altezze devono essere tali che l'area di ogni rettangolo risultante sia uguale alla frequenza relativa o assoluta della classe stessa.
È possibile creare un'istogramma in R utilizzando la funzione *hist()*. 
Questo comando prevede diverse opzioni interessanti: 

* *freq*, che può valere FALSE indicando così di utilizzare la frequenza relativa, oppure può essere impostato a TRUE per utilizzare la frequenza assoluta. Di default è impostato a TRUE se e solo se le classi sono equidistanti.

```{r}
h <- hist(famiglia, freq = TRUE, col = 1:5, 
          main="Istogramma per motivi di famiglia", 
          ylab="Frequenza assoluta classi", xlab = "Classi")
```

Utilizzando il comando *str()* R è in grado di fornirci informazioni aggiuntive sulla creazione dell'istogramma, come la suddivisione in classi utilizzata.

```{r}
str(h)
```

Com'è possibile vedere, le informazioni fornite da R riguardo l'istogramma sono diverse:

* *breaks* è un vettore contenente la suddivisione in classi utilizzata
* *counts* è un vettore i quali valori indicano il numero di elementi che ricade in ogni classe
* *density* è il vettore con le densità delle classi stimate
* *mids* vettore contenente i punti centrali delle classi
* *xname* è il nome del vettore di dati su cui si basa l'istogramma
* *equidist* un valore booleano per indicare se la distanza fra i valori di breaks è uguale

Da queste informazioni è possibile ricavare la frequenza relativa moltiplicando il vettore *density* per l'ampiezza delle classi.
In questo caso, l'ampiezza delle classi è 5.

```{r}
fr <- h$density * 5
fr
```

Utilizziamo adesso il vettore delle classi definito precedentemente durante il calcolo delle frequenze per definire un nuovo istogramma.

```{r}
h <- hist(famiglia, freq = TRUE, col = 1:5, 
          main="Istogramma per motivi di famiglia", 
          ylab="Frequenza assoluta classi", xlab = "Classi", 
          breaks = classi)
```

### Boxplot

Consideriamo il campione dei valori assunti da una variabile quantitativa "Lavoro". Ordinando il vettore in ordine crescente, è possibile calcolare dei particolari valori chiamati *quartili*. Prende il nome di **primo quartile** e  si indica con $Q_{1}$, il valore per il quale il 25% dei dati si trova alla sua sinistra e il restante 75% alla sua destra. In maniera speculare, il **terzo quartile**, indicato con $Q_{3}$ è il valore per il quale il 25% dei dati sono alla sua destra e il 75% alla sua sinistra. Il **secondo quartile** indicato con $Q_{2}$ è il valore che alla sua destra il 50% dei dati e alla sua sinistra il restante 50% ed è chiamato anche **mediana**. I valori $Q_{0}$ e $Q_{1}$ rappresentano rispettivamente il valore *minimo* e il valore *massimo* del campione. 
R fornisce le funzioni *quantile()* e *summary()* che ci permettono i valori del minimo ($Q_{0}$) e del massimo ($Q_{4}$), della mediana ($Q_{2}$), del primo e del terzo quartile ($Q_{1}$ e $Q_{3}$).
Riferendoci al vettore *lavoro*:

```{r}
quantile(lavoro)
```

```{r}
summary(lavoro)
```

Calcolati i quartili è possibile definire cos'è un boxplot. Un **boxplot**, detto anche diagramma a scatola con baffi, è un grafico relativo a caratteri quantitativi ottenuto a partire dai quartili. Il suo scopo è quello di descrivere le caratteristiche salienti della distribuzione. Il disegno della scatola ha come estremi $Q_{1}$ e $Q_{3}$ ed ha una linea orizzontale in corrispondenza di $Q_{2}$. Sia in alto che in basso sono presenti due linee orizzontali dette *baffi*. Il baffo inferiore corrisponde al valore più piccolo fra le osservazioni ed è maggiore o uguale del valore $Q_{1}$ - 1.5 $*$ ($Q_{3}$ - $Q_{1}$), mentre il baffo superiore corrisponde al valore più alto delle osservazioni ed è minore o uguale del valore $Q_{3}$ + 1.5 $*$ ($Q_{3}$ - $Q_{1}$). La distanza tra il primo e il terzo quartile è detta **scarto interquartile** o **intervallo interquartile**. Se tutti i valori cadono all'interno dell'intervallo (a,b) prendiamo il baffo inferiore come minimo e il baffo superiore come massimo. I numeri che non cadono nell'intervallo e risultano quindi essere fuori sono detti *valori anomali* o *outlier* e necessitano di uno studio per capirne le origini. 
Lo scopo del boxplot è quello di illustrare alcune caratteristiche di una distribuzione di frequenza come:

* la centralità
* la forma
* la dispersione
* la presenza di valori anomali

La centralità viene espressa dalla mediana. È possibile verificare la simmetria della distribuzione confrontando le differenze fra $Q_{3}$ - $Q_{2}$ e $Q_{2}$ - $Q_{1}$: se queste infatti risultano essere uguali o vicine allora la mediana si trova più o meno al centro della distribuzione, garantendo la simmetria dei dati.
I valori dei dati che sono al di sopra del baffo superior o al di sotto del baffo inferiore sono outliers e nel grafico vengano rappresentati sottoforma di punti.
Per la creazione di boxplot, R fornisce la funzione *boxplot()* . Il grafico può essere creato sia con orientamento verticale che orizzontale utilizzando il parametro *horizontal* che  di default è impostato a FALSE, indicante orientamento verticale. 

```{r}
boxplot(lavoro, col = "orange", main="Boxplot tasso cittadini 
        non comunitari residenti per motivi di lavoro")
```

Come descritto precedentemente, gli estremi della scatola nel grafico sono rappresentati da $Q_{3}$ = 32.08 e da $Q_{1}$ = 23.57 mentre la linea orizzontale, la mediana, è in corrispondenza di $Q_{2}$ = 28.25 . Il baffo inferiore è pari a 10.90 ovvero al primo numero maggiore o uguale a $Q_{1}$ - 1.5 $*$ ($Q_{3}$ - $Q_{1}$), 23.7 - 1.5 $*$ (32.08 - 23.57) = 10.93. Il baffo superiore invece è 41.60 ovvero al primo valore del campione minore o uguale di $Q_{3}$ + 1.5 $*$ ($Q_{3}$ - $Q_{1}$), ossia 32.08 + 1.5 $*$ (32.08 - 23.57) = 44.845. Tutti i valori ricadono fra il minimo e il massimo quindi non abbiamo anomalie. Per quanto riguarda la simmetria abbiamo che $Q_{3}$ - $Q_{2}$, 32.08 - 28.25 = 3.5, e $Q_{2}$ - $Q_{1}$, 28.25 - 23.57 = 4.68. I valori sono diversi ma molto vicini. 

I boxplot per le restanti modalità sono:

```{r}
quantile(famiglia)
```

```{r}
summary(famiglia)
```


```{r}
boxplot(famiglia, col = "red", main="Boxplot tasso cittadini 
        non comunitari residenti per motivi di famiglia")
```

In questo boxplot la mediana è spostata più verso l'alto, fattore che ci dice che la distribuzione non è simmetrica.

```{r}
quantile(studio)
```

```{r}
summary(studio)
```

```{r}
boxplot(studio, col = "green", main="Boxplot tasso cittadini 
        non comunitari residenti per motivi di studio")
```


```{r}
quantile(asilo)
```

```{r}
summary(asilo)
```

```{r}
boxplot(asilo, col = "yellow",main="Boxplot tasso cittadini 
        non comunitari residenti per motivi di asilo")
```
In questo boxplot è presente un valore anomale e, riferendoci alla tabella, scopriamo che è il Molise con 67.1. I baffo superiore è molto più grande di quello inferiore, dicendoci che la disperisione dei dati è maggiore. Anche la mediana non risulta essere al centro ma tende verso il basso incidendo sulla simmetria.

```{r}
quantile(altro)
```

```{r}
summary(altro)
```

```{r}
boxplot(altro, col = "purple", main="Boxplot tasso cittadini 
        non comunitari residenti per altri motivi")
```

Anche in quest'ultimo boxplot è presente un valore anomale che esce al di fuori dell'intervallo. In questo caso, la regione è il Lazio con 9.79. La mediana risulta essere più in basso rispetto al centro.

Possiamo anche confrontare i diversi boxplot mettendoli tutti insieme in un unico grafico.

```{r}
boxplot(lavoro, famiglia, studio, asilo, altro, 
        col = c("orange","red","green","yellow", "purple"), 
        names = c("lavoro", "famiglia", "studio", "asilo", "altro"))
```

### Scatterplot

Gli **scatterplot** o **diagrammi di dispersione** consistono in rappresentazioni grafiche delle relazioni tra variabili quantitative. Quello che si fa è fissare la variabile da porre sull'asse delle ascisse e quella da porre sull'asse delle ordinate. Queste variabili vengono chiamate rispettivamente variabile **indipendente** e variabile **dipendente**. Iniziamo col creare quindi un data frame, un tipo di struttura dati a disposizione in R. All'interno di questo data frame inseriamo i vettori delle diverse modalità. 

```{r}
motivi <- data.frame(lavoro, famiglia, studio, asilo, altro)
```

Per realizzare uno scatterplot basta utilizzare la funzione *pairs()* darle in input il data frame appena creato. 

```{r}
pairs(motivi, main="Scatterplot per le coppie di variabili")
```

Le immagini mostrano le nuvole di punti ottenute considerando tutte le coppie di variabili. Almeno nella metà delle coppie, è presente una qualche relazione, anche accennata. 
Alcune invece, come il plot che mette in relazione lavoro e famiglia, non sembrano essere in relazione. 
È possibile anche stampare uno solo grafico alla volta utilizzando la funzione *plot()* dandole in input singoli vettori del data frame.

```{r}
plot(motivi$lavoro, motivi$famiglia, xlab="Lavoro", 
     ylab="Famiglia", 
     main="Motivi di lavoro in relazione ai motivi di famiglia")
```

\newpage

## Statistica descrittiva univariata

Per i fenomeni quantitativi è utile definire la funzione di distribuzione empirica. Abbiamo due tipi di funzione di distribuzione empirica:

* funzione di distribuzione empirica **discreta**
* funzione di distribuzione empirica **continua**

### Funzione di distribuzione empirica discreta

Una **funzione di distribuzione empirica discreta** è una funzione utilizzata quando la nostra variabile assume valori discreti. Il fatto che i valori della variabile siano discreti, porta la funzione ad assumere una forma a gradini. Utilizziamo le frequenze relative $f_{i}$ = $\frac{n_{i}}{n}$ con i = 1, 2, ..., k  e le frequenze relative cumulate $F_{i}$ =  $f_{1}$ + $f_{2}$ + ... +$f_{k}$ dove k è il numero discreto di valori che il nostro campione di dati può assumere.
La funzione di distribuzione empirica discreta è così definita:


$$F(x) = \frac{\#\{x_i \leq x,i = 1,2,...,n\} }{n} = \begin{cases} 0,\quad x<z_1\\ F_1, \quad  z_1\leq x<z_2 \\ ... \\ F_i, \quad z_i \leq x < z_{i+1} \\ ... \\ 1, \qquad x \geq x_k\end{cases} $$

dove:

- *#* è la cardinalità dell'insieme
- gli $z_{i}$ rappresentano i possibili valori assunti dal campione 
- gli $x_{i}$ i valori del campione
- gli $F_{i}$ la proporzione dei dati del campione minori o uguali di $z_{i}$

La funzione di distribuzione empirica discreta F(x) è definita per ogni x reale. 
R dispone della classe *stepfun* che implementa una serie di metodi per trattare funzioni a gradino. In particolare, la funzione *ecdf()*, acronimo di *empirical cumulative distribution function*, permette di disegnare il grafico della funzione di distribuzione empirica per le variabili quantitative discrete.

```{r}
plot(ecdf(famiglia), main = "Funzione di distribuzione empirica discreta")
```

I gradini sono tanti quanto i valori assunti dalla variabile e, siccome in questo vettore i numeri sono tutti differenti tranne che per una coppia, ha poco senso creare questo grafico. 
Prendendo il vettore studio però ed eliminando la parte decimale è possibile visualizzare un esempio di questo grafico.

```{r}
plot(ecdf(round(studio, digits = 0)), 
     main="Funzione di distribuzione empirica discreta", col="red")
```

La funzione *round()* serve ad arrotondare un numero alla cifra specificata con il parametro *digits*.

### Funzione di distribuzione empirica continua

Per fenomeni quantitativi continui, la funzione di distribuzione empirica è una funzione **continua**, necessaria se i dati vengono raccolti in classi. Se i dati vengono quindi raccolti in k distinte classi $C_{1}$ = [$z_{1}$, $z_{2}$), $C_{2}$ = [$z_{3}$, $z_{4}$), ..., $C_{k}$ = [$z_{k}$, $z_{k+1}$],
la funzione di distribuzione empirica è così definita:

$$F(x) = \begin{cases} 0, \quad x<z_1 \\ ... \\ F_i, \quad x=z_i \\ \frac{F_{i+1} - F_i}{z_{i+1}-z_i}x + \frac{z_{i+1}F_i - z_iF_{i+1}}{z_{i+1}- z_i}, \quad z_i < x < z_{i+1}, \quad z_i<x<z_{i+1} \\ F_{i+1}, \quad x= z_{i+1} \\ ... \\ 1, \quad x \geq z_{k+1}\end{cases}$$

Si noti che F(x) = 0 per $x<z_{1}$ , F(x) = 1 per  $x\leq z_{k+1}$ mentre se $z_{i}< x <z_{i+1}$ la funzione empirica coincide con il segmento che passa per i punti ($z_{i}$, $F_{i}$) e ($z_{i+1}$,$F_{i+1}$)

La suddivisione in classi adottata è la seguente: [0, 15), [15, 20), [20, 25)

```{r}
classi_empiriche <- c(0,15,25,30,35,40,45,50,55,60,70)

Ffamiglia <- cumsum(table
                    (cut
                      (famiglia, breaks=classi_empiriche, right =FALSE)))/length(famiglia)
Ffamiglia
```

Il vettore Fi contiene le frequenze relative cumulate dei tassi delle varie regioni associati alle varie classi.

```{r}
Ffamiglia <- c(0, Ffamiglia)
plot(classi_empiriche, Ffamiglia, type = "b", 
     axes = FALSE, main = "Funzione di distribuzione empirica continua", 
     ylab ="Funzione", xlab = "Classi", col = "red")
axis(1, classi_empiriche)
axis(2, format(Ffamiglia, digits=2))
box()
```

Utilizziamo $c(0, F_i)$ per aggiungere uno zero all'inizio del vettore delle frequenze relative cumulate. Nella funzione *plot()* abbiamo utilizzato l'opzione *type = b* che ci consente di congiungere i punti successivi del grafico mediante delle linee continue ai cui estremi troviamo dei piccoli cerchi. L'opzione *axes = FALSE* consente di non tracciare gli assi. Mediante l'opzione *axis(1, classi)* si disegna l'asse orizzontale in basso e con *axis(2, format(Fi, digits=2))* si ottiene l'asse verticale di sinistra con una formattazione opportuna dei numeri. Con *box()* racchiudiamo tutto in un rettangolo. 

Utilizzando la stessa suddivisione in classi, creiamo il grafico per i vettori asilo e lavoro:

```{r}
Flavoro <- cumsum(table
                  (cut
                    (lavoro, breaks=classi_empiriche, right =FALSE)))/length(lavoro)
Flavoro <- c(0, Flavoro)
plot(classi_empiriche, Flavoro, type = "b", 
     axes = FALSE, main = "Funzione di distribuzione empirica continua", 
     ylab ="Funzione", xlab = "Classi", col = "red")
axis(1, classi_empiriche)
axis(2, format(Flavoro, digits=2))
box()
```

```{r}
Fasilo <- cumsum(table
                 (cut
                   (asilo, breaks=classi_empiriche, right =FALSE)))/length(asilo)
Fasilo <- c(0, Fasilo)
plot(classi_empiriche, Fasilo, type = "b", 
     axes = FALSE, main = "Funzione di distribuzione empirica continua", 
     ylab ="Funzione", xlab = "Classi", col = "red")
axis(1, classi_empiriche)
axis(2, format(Fasilo, digits=2))
box()
```

Siccome i vettori studio e altro risultano contenere valori decisamente piccoli, utilizziamo una suddivisione in classi diversa:

```{r}
classi_empiriche1 <- c(0,2,4,6,8,10)
Fstudio <- cumsum(table
                  (cut
                    (studio, breaks=classi_empiriche1, right =FALSE)))/length(studio)
Fstudio <- c(0, Fstudio)
plot(classi_empiriche1, Fstudio, type = "b", 
     axes = FALSE, main = "Funzione di distribuzione empirica continua", 
     ylab ="Funzione", xlab = "Classi", col = "red")
axis(1, classi_empiriche1)
axis(2, format(Fstudio, digits=2))
box()
```

```{r}
Faltro <- cumsum(table
                 (cut
                   (altro, breaks=classi_empiriche1, right =FALSE)))/length(altro)
Faltro <- c(0, Faltro)
plot(classi_empiriche1, Faltro, type = "b", 
     axes = FALSE, main = "Funzione di distribuzione empirica continua", 
     ylab ="Funzione", xlab = "Classi", col = "red")
axis(1, classi_empiriche1)
axis(2, format(Faltro, digits=2))
box()
```

### Indici di sintesi
Come visto prima, attraverso i boxplot è possibile rappresentare in maniera semplice la distribuzione di frequenza di un campione di dati numerico. Quello che faremo adesso è introdurre degli indici che servono a misurare quantitativamente alcune delle caratteristiche osservate nei boxplot e nei grafici di distribuzioni di frequenza.
Gli indici di **sintesi**, detti anche *statistiche*, sono utili nel descrivere dati numerici. In particolare questi indici sono: *media*, *moda*, *mediana*, *varianza*, *deviazione standard* e *coefficiente di variazione*. Le prime tre, media, mediana e moda, sono *misure di centralità* mentre le restanti tre misurano la *dispersione dei dati*. 

### Media, mediana e moda

#### Media

Supponiamo di avere a disposizione un insieme $x_{1}, x_{2}, ... ,x_{n}$ di n valori, chiamato *campione di ampiezza n*. La **media campionaria**, indicata con **$\bar{x}$** è la media aritmetica di questi valori.

$$\overline{x} = \frac{1}{n}\sum_{i=1}^n x_i$$

Questa misura gode della proprietà di linearità. Inoltre, è possibile vedere la media come una media pesata dei valori distinti assunti dai dati. Ogni valore distinto usa come peso la sua frequenza relativa, ovvero la frazione dei dati uguale al suo valore. 

Per ogni valore $x_{i}$ si definisce lo *scarto della media campionaria* la quantità 

$s_{i} = x_{i}-\bar{x}$ con $(i = 1,2,...,n)$

che indica il grado di scostamento del singolo valore $x_{i}$ dalla media campionaria $\bar{x}$.
Si noti che la somma algebrica degli scarti della media campionaria è sempre *nulla*, risulta infatti che:

$$\sum_{i=1}^n s_i = \sum_{i=1}^n (x_i - \overline{x}) = \sum_{i=1}^n x_i - n\overline{x} = \overline{x} - \overline{x} = 0 $$

Va sottolineato che la media campionaria è sensibile ai valori anomali.

Per il calcolo della media campionaria, R fornisce la funzione *mean()*

```{r}
mean(lavoro)
```

```{r}
mean(famiglia)

```

```{r}
mean(studio)
```

```{r}
mean(asilo)
```

```{r}
mean(altro)
```

#### Mediana

Una seconda statistica che indica la centralità di un insieme di dati è la **mediana campionaria**. Assegnato un insieme di dati di ampiezza n, lo si ordini dal minore al maggiore. Se n è dispari, si definisce mediana campionaria il valore che è in posizione $\frac{(n+1)}{2}$, mentre se n risulta essere pari la mediana campionaria è definita come la media aritmetica dei valori che occupano le posizioni $\frac{n}{2}$ e $\frac{(n+1)}{2}$. Questa definizione di mediana campionaria bipartisce le osservazioni in due gruppi di uguale numerosità.
Il linguaggio R ci mette a disposizione la funzione *median()* per calcolare la mediana di un vettore contenente delle osservazioni. 
Di seguito, la funzione *median()* viene utilizzata per calcolare la mediana dei vettori a nostra disposizione:

```{r}
median(lavoro)
```

```{r}
median(famiglia)
```

```{r}
median(studio)
```

```{r}
median(asilo)
```

```{r}
median(altro)
```

Sia la media campionaria che la mediana campionaria risultano essere statistiche utili per descrivere la centralità dei dati. A dispetto però della media campionaria, la mediana dipende solo da uno o due dei valori centrali dei dati e non risente dei valori estremi (estremamente bassi o alti). Lo svantaggio di utilizzare la mediana è che bisogna prima ordinare i dati a dispozione prima di poterla calcolare.

### Moda

La terza statistica utilizzata per descrivere la centralità dei dati è la **moda campionaria**. La moda campionaria di un insieme di dati, se esiste, è la modalità a cui è associata la frequenza (assoluta o relativa) più elevata. Se esistono più modalità con frequenza massima, ciascuna di esse è detta **valore modale**. La moda quindi nient'altro è che il valore che si presenta con frequenza maggiore nell'insieme dei dati. Per definizione, è possibile utilizzare la moda anche per dati di tipo qualitativo, cosa che non è possibile fare con media e mediana. La moda può non esistere oppure non essere unica: quando non è unica, la distribuzione è detta *unimodale* mentre quando ci sono due o più mode diverse è detta *bimodale* o *multimodale*.
Non esiste in R una funzione per estrarre la moda da una distribuzione di dati poichè essa risulta facilmente estraibile osservando il grafico delle frequenze assolute. 
Si è optato lo stesso per definire una funzione in grado di ricavarla:

```{r}
moda <- function(v){
  y<-table(v)
  z<-which(y==max(y))
  return(c(z))
}
```

Applichiamo poi la moda a ciascun vettore della matrice:

```{r}
moda(lavoro)
```

```{r}
moda(famiglia)
```

```{r}
moda(studio)
```

```{r}
moda(asilo)
```

```{r}
moda(altro)
```

In quasi tutti i vettori considerati, la moda è risultata essere una buona statistica per misurare la centralità tranne nell'ultimo caso, col vettore *altro*, dove i dati sono molto diversi fra loro.

Dopo aver calcolato queste tre statistiche, è possibile descrivere la forma di una distribuzione confrontando la media campionaria e la mediana campionaria. Se queste due statistiche risultano essere uguali allora la distribuzione di frequenza tende ad essere simmetrica; se la media campionaria è maggiore della mediana campionaria allora la distribuzione sarà sbilanciata verso destra; se invece la media campionaria è sensibilimente minore della mediana campionaria allora la distribuzione di frequenza è più sbilanciata verso sinistra. 

## Mediana per distribuzione di frequenza

Un modo di procedere diverso per definire la mediana è quello di considerare le frequenze relative cumulate.
Sia X una variabile quantitativa e siano $z_{1} < z_{2} < ... < z_{k}$.
Considerato un campione ($x_{1}, x_{2}, ..., x_{n}$), siano $F_{i} = f_{1} + f_{2} + ... + f_{i}$ con $(i = 1, 2, ..., k)$ le frequenze relative cumulate.

La **mediana per una distribuzione di frequenze** è definita come la modalità i-esima $(i = 1, 2, ..., k)$ che soddisfa la doppia disuguaglianza:
$F_{i-1} < 0.5$, $F_{i} \geq 0.5$

Come si evince dalla definzione, la mediana di una distribuzione di frequenza è un valore di sintesi che indica un punto centrale intorno al quale si dispone la distribuzione di frequenza.

```{r}
par(mfrow = c(2,2))
plot(classi_empiriche, Flavoro, type="b", 
     main="Funzione di distribuzione empirica \n discreta di lavoro", col="red")
abline(h=0.5, lty=2, col="blue")
plot(classi_empiriche, Ffamiglia, type="b", 
     main="Funzione di distribuzione empirica \n discreta di famiglia", col="red")
abline(h=0.5, lty=2, col="blue")
plot(classi_empiriche, Fasilo, type="b", 
     main="Funzione di distribuzione empirica \n discreta di asilo", col="red")
abline(h=0.5, lty=2, col="blue")
plot(classi_empiriche1, Fstudio, type="b", 
     main="Funzione di distribuzione empirica \n discreta di studio", col="red")
abline(h=0.5, lty=2, col="blue")
plot(classi_empiriche1, Faltro, type="b", 
     main="Funzione di distribuzione empirica \n discreta di altro", col="red")
abline(h=0.5, lty=2, col="blue")
```

Per ottenere il valore della mediana per la distribuzione di frequenze utilizziamo la funzione *quantile()*.

```{r}
quantile(lavoro, 0.5, type=1)
```
```{r}
quantile(famiglia, 0.5, type=1)
```

```{r}
quantile(studio, 0.5, type=1)
```

```{r}
quantile(asilo, 0.5, type=1)
```

```{r}
quantile(altro, 0.5, type=1)
```

### Quantili, percentili, decili e quartili

Oltre la mediana è possibile definire altri indici di posizione detti **quantili**, che dividono l'insieme dei dati ordinati in un numero di parti uguali. Supponiamo di avere a disposizione una variabile quantitativa con un certo numero di osservazioni disposte in ordine crescente. Possiamo dividere i dati in $\alpha$ gruppi, ognuno contenente circa lo stesso numero di elementi. Gli $\alpha-1$ numeri sono detti *quantili* di ordine $\alpha$. Solitamente possiamo dividere i dati in $\alpha = 4$ gruppi da 3 quantili detti *quartili*, oppure $\alpha = 10$ e 9 quantili detti *decili* o ancora $\alpha = 100$ e 99 quantili detti *percentili*. I quantili (percentili) sono indici di posizione non centrali utilizzati per insiemi numerosi di dati. 
Per il calcolo dei quantili R ha a disposizione la funzione *quantile(v, probs = , type = )*. Oltre al vettore, la funzione prende in input anche due ulteriori parametri:

* *probs* che rappresenta il vettore delle probabilità
* *type* a cui è possibile assegnare valori da 1 a 9 e indica l'algoritmo da utilzzare per il calcolo dei quantili. Infatti, R possiede per 9 algoritmi per il calcolo dei quantili e quello di default è il 7, basato su tecniche di interpolazione tra i punti. 

```{r}
quantile(lavoro)
```

```{r}
quantile(famiglia)
```

```{r}
quantile(studio)
```

```{r}
quantile(asilo)
```

```{r}
quantile(altro)
```

Abbiamo detto che R è in grado di utilizzare diversi algoritmi per il calcolo dei quantili. Mostriamo ora lavorando sul vettore famiglia, la differenza fra i vari algoritmi

```{r}
tipiquartili <- function(x){
  y<-numeric(0)
  for(i in 1:9){
    y<-rbind(y, c(quantile(x, 0, type=i), quantile(x, 0.25, type=i), 
                  quantile(x, 0.5, type=i), quantile(x, 0.75, type=i),
                  quantile(x, 1, type=i)))
  }
  rownames(y) <- paste("type", 1:9)
  y
}
tipiquartili(famiglia)
```

Calcolando i quantili sul vettore famiglia con i differenti algoritmi è possibile notare che i risultati restituiti differiscono.

### Varianza, deviazione standard e coefficiente di variazione
Siccome gli indici di posizione non tengono conto della variabilità dei dati, abbiamo bisogno di introdurre due nuovi indici: *varianza campionaria* e *deviazione standard*. Questi due indici ci permettono di permettono di misurare la variabilità di una distribuzione di frequenza.
Diamo le definizioni di queste due misure:

Assegnato un insieme di dati numerici $x_{1}, x_{2}, ..., x_{n}$, si definisce **varianza campionaria**, indicata con $s^{2}$, la quantità:

$$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_{i} - \bar{x})$$ con n=2,3,...

dove $\bar{x}$ è la media campionaria. 

Definiamo come **deviazione standard**, indicata con s, la radice quadrata della varianza campionaria
$$s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_{i} - \bar{x})}$$ con n=2,3,...

Questi due indici sono detti **indici di dispersione** in quanto più i dati risultano discostarsi dalla media più essi risultano essere grandi.

La divisione per $n-1$ è un'inferenza statistica e serve per evitare di calcolare su un vettore di ampiezza 1. 

In R è possibile calcolare la varianza campionaria di un vettore numerico utilizzando la funzione *var()*:

```{r}
var(lavoro)
```
```{r}
var(famiglia)
```
```{r}
var(studio)
```
```{r}
var(asilo)
```
```{r}
var(altro)
```

È possibile ricavare la devizione standard campionaria utilizzando la funzione *sd()*:

```{r}
sd(lavoro)
```
```{r}
sd(famiglia)
```
```{r}
sd(studio)
```
```{r}
sd(asilo)
```
```{r}
sd(altro)
```

La media campionaria e la varianza campionaria sono, rispettivamente, gli indici di posizione e di dispersione più utilizzati.
Per confrontare le variazioni esistenti tra i diversi campioni di dati è possibile utilizzare il **coefficiente di variazione**.

Assegnato un insieme di dati numerici $x_{1}, x_{2}, ...,x_{n}$ si definisce *coefficiente di variazione* il rapporto tra la deviazione standard e il modulo della media campionaria:

$$ CV = \frac{s}{|\bar{x}|}$$

In R non è presente una funzione per calcolare il coefficiente di variazione, quindi la definiamo noi:

```{r}
cv <- function(x){
  sd(x)/abs(mean(x))
}
```

Applicandola ai vettori a nostra disposizione:

```{r}
cv(lavoro)
```

```{r}
cv(famiglia)
```

```{r}
cv(studio)
```

```{r}
cv(asilo)
```

```{r}
cv(altro)
```

Tra i coefficienti di variazione calcolati, gli ultimi tre risultano essere quelli con valore maggiore, fatto che indica che la dispersione dei dati è maggiore in questi vettori.

### Forma di una distribuzione di frequenza

Con gli indici descritti finora è possibile comprendere la forma delle distribuzioni di frequenza ed eventuali sbilanciamenti verso destra o sinistra. Esistono indici statistici che permettono di misurare quando una distribuzione di frequenza presenta simmetria o asimmetria oppure se è più o meno piccata. 
Un indice che permette di misurare la simmetria di una distribuzione di frequenza è la **skewness campionaria**.

Assegnato un insieme di dati numerici $x_{1}, x_{2}, ..., x_{n}$ si definisce *skewness campionaria* il valore: 

$$ \gamma_{1} = \frac{m_{3}}{m_{2}^{3/2}}$$

dove $m_{3}$ denota il momento centrato campionario di ordine 3. 
In generale, il momento centrato campionario di ordine j è definito come segue:

$$ m_{j} = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - \bar{x})^{j} \qquad j=1,2,...$$

Se $\gamma_{1}=0$ allora la distribuzione di frequenza è simmetrica;
Se $\gamma_{1}>0$ allora l'asimmetria è positiva (la coda di destra è più allungata);
Se $\gamma_{1}<0$ l'asimmetria è negativa (la coda di sinistra è più allungata).

Non è presente una funzione per il calcolo della skewness campionaria in R, quindi abbiamo provveduto ad implementarlo:

```{r}
skw <- function(x){
  n <- length(x)
  m2 <- (n-1)*var(x)/n
  m3 <- (sum((x-mean(x))^3))/n
  m3/(m2^1.5)
}
```

Riferendoci ai dati dei vettori in analisi,

```{r}
skw(lavoro)
```

Qui abbiamo un'asimmetria negativa in quanto la skewness è minore di 0. 

```{r}
skw(famiglia)
```

Anche qui abbiamo un'asimmetria negativa in quanto la skewness è minore di 0. 

```{r}
skw(studio)
```

```{r}
skw(asilo)
```

```{r}
skw(altro)
```

Con questi ultimi tre vettori, l'asimmetria risulta essere positiva in quanto la skewness è maggiore di 0. 

Un indice per misurare la densità dei dati intorno alla media è la **curtosi campionaria**

Assegnato un insieme di dati numerici $x_{1, x_{2}, ...,x_{n}}$ si definisce *curtosi campionaria* il valore:

$$\gamma_{2} = \beta_{2} - 3$$
dove

$$\beta_{2} = \frac{m_{4}}{m_{2}^{2}}$$

dove $m_{4}$ rappresenta il momento campionario di ordine 4.

Questi due indici $\beta_2$ e $\gamma_2$ permettono di confrontare la distribuzione di frequenza dei nostri dati con una densità di probabilità normale.

Se $\beta_2<3$ e $\gamma_2<0$ la distribuzione di frequenza si dice *platicurtica*, ovvero risulta essere più piatta di una normale.
Se $\beta_2>3$ e $\gamma_2>0$ la distribuzione di frequenza si dice *leptocurtica*, ovvero risulta essere più piccata di una normale.
Se $\beta_2 = 3$ e $\gamma_2 = 0$ la distribuzione di frequenza si dice *normocurtica*, ovvero piatta come una normale.

Implementiamo in R il calcolo della curtosi campionaria come segue

```{r}
curt <- function(x){
  n<-length(x)
  m2<-(n-1)*var(x)/n
  m4<-(sum((x-mean(x))^4))/n
  m4/(m2^2) - 3
}
```

Riferendoci ai vettori in analisi

```{r}
curt(lavoro)
```

```{r}
curt(famiglia)
```

```{r}
curt(studio)
```

```{r}
curt(asilo)
```

```{r}
curt(altro)
```

Nei vettori *lavoro*, *asilo* e *altro* la curtosi è positiva indicando quindi che la distribuzione di frequenza è più piccata di una normale e si dice in questo caso leptocurtica. Per quanto riguarda i vettori *famiglia* e *studio* invece, la curtosi è negativa, ovvero la distribuzione è più piatta di una normale, quindi platicurtica.

## Statistica descrittiva bivariata

La statistica descrittiva bivariata è la branca della statistica che si occupa di definire i metodi grafici e statistici per descrivere le relazioni fra due variabili. Le relazioni fra variabili quantitative possono essere rappresentate graficamente mediante scatterplot in cui, ogni coppia di osservazioni viene rappresentata con un punto o un cerchio sul piano euclideo. Quello che si fa è scegliere una variabile da porre sull'asse delle ascisse, che prende il nome di *variabili indipendente*, e una da porre sull'asse delle ordinate chiamata *variabile dipendente* e si disegnano i punti in corrispondenza delle coppie. Creando questo grafico vogliamo capire se le coppie di punti presentano una certa regolarità e se esiste una relazione fra le variabili.

```{r}
plot(famiglia, asilo, main="Asilo in funzione di famiglia", 
     xlab="Famiglia", ylab = "Asilo")
abline(v=median(famiglia), lty=1, col="magenta")
abline(v=mean(famiglia), lty=2, col="blue")
abline(h=median(asilo), lty=1, col="magenta")
abline(h=mean(asilo), lty=2, col="blue")
```

Dallo scatterplot prodotto possiamo notare che i dati sembrano essere sparsi intorno ad una retta discendente, mostrando così una correlazione lineare negativa fra le variabili.

### Covarianza e correlazione campionaria

È possibile misurare la correlazione tra le variabili utilizzando la **covarianza campionaria**.

Assegnato un campione bivariato ($x_1, y_1$), ($x_2, y_2$), ..., ($x_n, y_n$) di una variabile quantitativa bidimensionale (X,Y), siano $\bar x$ e $\bar y$ le medie campionarie di $x_1, x_2, ..., x_n$ e di $y_1, y_2, ..., y_n$ . La covarianza campionaria tra le due variabili X e T è definita come segue:

$$ C_{xy} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar x)(y_i - \bar y)$$

Quando $C_{xy} > 0$ si dice che le variabili sono *correlate positivamente*.
Quando $C_{xy} < 0$ si dice che le variabili sono *correlate negativamente*.
Quando $C_{xy} = 0$ si dice che le variabili sono *non  correlate*.

Inoltre dividiamo per $n-1$ perchè in caso x e y siano uguali la covarianza campionaria è uguale alla varianza campionaria.

È possibile calcolare la covarianza campionaria in R utilizzando la funzione *cov()*

```{r}
cov(famiglia, asilo)
```

Com'è possibile vedere, la covarianza campionaria fra le due variabili considerate è negativa, fatto che ci dice che le due variabili sono correlate negativamente.

È possibile ottenere una misura quantitativa della correlazione fra le variabili introducendo il **coefficiente di correlazione campionaria**

$$r_{xy} = \frac{C_{xy}}{s_x s_y}$$
dove $s_x$ e $s_y$ corrisponde rispettivamente alla deviazione standard di x e di y.

Questo coefficiente di correlazione ha lo stesso segno della covarianza e, in base al suo valore, diciamo che le variabili sono correlate positivamente, negativamente o non correlate.

Il coefficiente di correlazione gode di diverse proprietà:

- $-1 \leq r_{xy} \leq 1$;
- Se $r_{xy}=1$ allora abbiamo una correlazione perfetta positiva e tutti i punti sono allineati su una linea retta ascendente;
- Se $0 < r_{xy} < 1$ allora i punti sono posizionati in una nuvola attorno ad una linea retta interpolante ascendente;
- Se $r_{xy}=0$ allora i punti sono dispersi in una nuvola che non presenta alcuna evidente direzione di natura lineare;
- Se $-1 < r_{xy} < 0$ allora i punti sono posizionati in una nuvola attorno ad una linea retta interpolante discendente;
- Se $r_{xy} = -1$ abbiamo una correlazione perfetta negativa e tutti i punti sono allineati su una retta discendente.

In R per calcolare il coefficiente di correlazione si utilizza la funzione *cor()*

```{r}
cor(famiglia, asilo)
```

Misurando la correlazione fra le due variabili che stiamo considerando otteniamo un valore negativo piuttosto vicino a -1, fatto che indica la correlazione negativa fra le due variabili.

È possibile aggiungere allo scatterplot precedente la retta interpolante stimata.

```{r}
plot(famiglia, asilo, main = "Retta di regressione", 
     xlab="Famiglia", ylab="Asilo", col="red")
abline(lm(asilo~famiglia), col="blue")
```

La funzione *abline(lm(asilo~famiglia)* è quella che permette di aggiungere la retta allo scatterplot, dove *lm()* sta per linear model ed è utilizzata per eseguire le analisi di regressioni lineari.

Il modello lineare viene utilizzato per spiegare o prevedere un andamento futuro sulla base della relazione si instaura tra una variabile Y dipendente e una o più variabili
indipendenti $X_{1}, X_{2}, ..., X_{p}$. Nel caso in cui $p=1$, l'analisi prende il nome di **regressione semplice** mentre se $p>1$ allora prende il nome di **regressione multipla**.

### Regressione lineare semplice 

Questo modello di regressione è esprimibile attraverso l'equazione di una retta che interpola al meglio la nuvola di punti dello scatterplot. L'equazione è la seguente:

$$Y=\alpha+\beta X$$

dove:

- $\alpha$ è l'intercetta;
- $\beta$ il coefficiente angolare.

Il coefficiente angolare indica la pendenza della retta mentre l'intercetta corrisponde all'ordinata del punto di intersezione della retta interpolante con l'asse delle ordinate.
L'identificazione di questa retta viene ottenuta applicando il *metodo dei minimi quadrati*. I **coefficienti di regressione** sono i valori $\alpha$ e $\beta$ per i quali la somma $Q$ dei quadrati degli errori è **minima**:

$$Q = \sum_{i=1}^n [y_{i} - (\alpha+\beta x_i)]^2$$

dove:

- $n$ è il numero di osservazioni;
- $(x_1, x_2, ..., x_n)$ sono i valori osservati della variabile X;
- $(y_1, y_2, ..., y_n)$ sono i valori osservati della variabile Y.

Il metodo dei minimi quadrati conduce alla fine a:

$$ \beta = \frac{s_y}{s_x} * r_{xy} \qquad \alpha = \bar{y}-\beta \bar{x}$$

Riferendoci ai vettori da noi presi in considerazione abbiamo che:

```{r}
beta <- (sd(asilo)/sd(famiglia)) * cor(famiglia,asilo)
alpha <- mean(asilo) - beta*mean(famiglia)

c(alpha,beta)
```

Il fatto che $\beta$ risulta essere negativo, ci conferma che la retta è discendente.
Per poter eseguire le analisi di regressione lineare utilizziamo la funzione *lm(y~x)* (l'argomento indica che y dipende da x).

```{r}
linearmodel <- lm(asilo~famiglia)
linearmodel
```

Vediamo che i coefficienti coincidono con quelli calcolati precedentemente. 
La retta di regressione con i vettori da noi presi in esame ha come equazione:

$$ y = 76.016 - 1.269 * x$$

### Residui

Calcolati i coefficienti di regressione e con essi l'equazione della retta di regressione, è possibile osservare quanto questa retta si discosta dai valori osservati.
Esistono quindi degli scostamenti, chiamati **residui**, tra le ordinate dei punti $y_i$ che sono i valori osservati e i corrispondenti valori stimati ottenuti mediante la retta di regressione.

$$\hat{y} = \alpha + \beta x_{i} \qquad (i=1,2,...,n)$$

I residui verranno quindi calcolati nel seguente modo:

$$E_i = y_i - \hat{y_i} \qquad (i=1,2,...,n)$$

È possibile calcolare i valori stimati utilizzando la funzione *fitted()* dandole in input *lm(y~x*)

```{r}
fitted(lm(asilo~famiglia))
```

Per calcolare i residui invece esiste la funzione *resid()* che prende in input sempre *lm(y~x)*

```{r}
resid(lm(asilo~famiglia))
```

I residui sono anche uno degli attributi fornitoci dalla funzione *lm()* ed è possibile accedervi nella seguente maniera:
```{r}
linearmodel$residuals
```

È inutile calcolare la media campionaria dei residui $\bar{E}$ in quanto essa è sempre nulla perchè gli scostamenti positivi e negativi si compensano a vicenda. Per quanto riguarda invece mediana, varianza campionaria e deviazione standard campionaria:

```{r}
median(linearmodel$residuals)
```

```{r}
var(linearmodel$residuals)
```

```{r}
sd(linearmodel$residuals)
```


Il coefficiente di variazione è impossibile da ottenere in quanto abbiamo detto che la media campionaria dei residui risulta essere nulla.
È possibile visualizzare graficamente i residui attraverso diversi grafici:

* Tracciando dei segmenti che congiungono i valori stimati e i valori osservati utilizzando la funzione *segments()*

```{r}
plot(famiglia, asilo, main = "Retta di regressione",
     xlab="Famiglia", ylab="Asilo", col="red")
abline(lm(asilo~famiglia), col="blue")
stime <- fitted(lm(asilo~famiglia))
segments(famiglia, stime, famiglia, asilo, col="magenta")
```

* Rappresentando i valori dei residui rispetto alle osservazioni della variabile indipendente
Il **diagramma dei residui** è un grafico in cui i valori dei residui sono posti sull'asse delle ordinate e quelli della variabile indipendente sull'asse delle ascisse.
```{r}
residui <- resid(lm(asilo~famiglia))
plot(famiglia, residui, main = "Diagramma dei residui", 
     xlab="Famiglia", ylab="Residui", pch=9, col="red")
abline(h=0, col="blue", lty=2)
```

Questo diagramma ci aiuta a capire come si adatta la retta di regressione rispetto ai dati.

Calcola i valori dei residui standardizzati rispetto ai valori stimati
È possibile calcolare i *residui standardizzati* nella seguente maniera:

$$ E_i^{(s)} = \frac{E_i - E}{s_E}$$

che hanno media campionaria nulla e varianza unitaria.

```{r}
residuistandard <- residui/sd(residui)
residuistandard
```

È possibile rappresentare questi residui standardizzati mediante un grafico:

```{r}
plot(stime, residuistandard, 
     main = "Residui standard rispetto ai valori stimati", 
     xlab="Valori stimati", ylab="Residui standard",
     pch=5, col="red")
abline(h=0, col="blue", lty=2)
```

### Coefficiente di determinazione

Quello che ci interessa capire è quanto la retta si adatti ai dati e questo lo possiamo fare attraverso il **coefficiente di determinazione** definito nella seguente maniera:

$$D^2 = \frac{\sum_{i=1}^n (\hat{y_i} - \bar{y})²}{\sum_{i=1}^n (y_i - \bar{y})²}$$

Il coefficiente di determinazione è il rapporto tra *la varianza dei valori stimati tramite la retta di regressione* e *la varianza dei valori osservati*.

Nel caso di regressione lineare semplice esso è pari al quadrato del coefficiente di correlazione, ovvero:

$$D^2 = r^2_{xy}$$

Se il coefficiente di determinazione è vicino ad 1 allora fra le due variabili esiste una forte correlazione, se invece questo coefficiente risulta vicino allo 0 non esiste correlazione fra le due variabili.
In questo caso, per calcolare il coefficiente di determinazione, possiamo utilizzare il coefficiente di correlazione ed elevarlo al quadrato:

```{r}
(cor(famiglia, asilo))^2
```

In alternativa possiamo utilizzare la funzione *summary(lm(y~x))$r.square*:

```{r}
summary(lm(asilo~famiglia))$r.square
```

Com'é possibile vedere, il coefficiente di determinazione fra le due variabili prese in esame risulta essere abbastanza vicino ad 1, indicando una correlazione fra le variabili.

## Regressione lineare multipla

Molte applicazioni affrontano situazioni nelle quali vi è più di una variabile indipendente. Il modello di regressione lineare multipla serve a spiegare la relazione fra una variabile quantitativa dipendente $Y$ e diverse variabili quantitative indipendenti $X_1, X_2, ..., X_p$. Come variabile dipendente scegliamo "Lavoro" e ne valutiamo la relazione con tutte le altre variabili a nostra disposizione. Iniziamo con l'applicare le funzioni *cov()* e *cor()* sul dataframe "motivi" definito in precedenza, nella parte riguardante gli scatterplot, per calcolare la covarianza e le correlazioni fra le coppie di variabili. Gli output delle funzioni saranno due matrici simmetriche.

```{r}
cov(motivi)
```

```{r}
cor(motivi)
```

Dalle matrici restituiteci come risultato possiamo notare che vi è una correlazione negativa molto vicina ad 1 fra la variabile "Lavoro" e "Asilo" e, come visto prima, anche fra "Famiglia" e "Asilo" e "Studio" e "Asilo".

Il modello di regressione lineare multipla con p variabili è esprimibile attraverso l'equazione: 

$$ Y = \alpha + \beta_1 X_1 + \beta_2 X_2 +...+\beta_p X_p $$

dove:

- $\alpha$ rappresenta l'intercetta, ovvero il valore di Y quando $X_1=X_2=...=X_p=0$
- $\beta_1 , \beta_2, ..., \beta_p$ sono i regressori. Abbiamo che $\beta_1$ rappresenta l'inclinazione di $Y$ rispetto alla variabile $X_1$ tenendo costanti le variabili $X_2, X_3, ..., X_p$ , $\beta_2$ rappresenta l'inclinazione... e così fino a $\beta_p$.

Utilizziamo la funzione *lm(y~$x_1+x_2+...+x_p$)* per eseguire l'analisi di regressioni lineari multivariate:

```{r}
multiplelinearmodel <- lm(lavoro ~ famiglia + studio + asilo + altro) 
multiplelinearmodel$coefficients
```
I segni di tutti i regressori sono negativi, questo vuol dire che i valori di "Famiglia", "Studio", "Asilo"" e "Altro" hanno un effetto negativo sul valore di "Lavoro", in altre parole al crescere dei valori "Famiglia", "Studio", "Asilo"" e "Altro" diminuisce quello di "Lavoro".

### Residui

Calcolati i coefficienti passiamo al calcolo dei residui. Prima di calcolare i residui bisogna calcolare i valori stimati che risultano essere uguali a:

$$\hat{y_i} = \alpha + \beta_1 x_{i,1} + \beta_2 x_{i,2} +...+\beta_p x_{i,p} \qquad (i=1,2,...,n) $$
Ricordiamo che la media campionaria dei residui $\bar{E}$ è sempre nulla. I residui li calcoliamo esattamente come prima:

$$E_i = y_i - \hat{y_i}$$
Utilizziamo anche qui la funzione *fitted()* per calcolare i valori stimati tramite regressione lineare multipla:

```{r}
stimemult <- fitted(lm(lavoro ~ famiglia + studio + asilo + altro))
stimemult
```

```{r}
residuimult <- resid(lm(lavoro ~ famiglia + studio + asilo + altro))
residuimult
```

Per quanto riguarda mediana, varianza campionaria e deviazione standard dei residui abbiamo che:

```{r}
median(multiplelinearmodel$residuals)
```

```{r}
var(multiplelinearmodel$residuals)
```


```{r}
sd(multiplelinearmodel$residuals)
```

Anche nel caso multivariato è interessante calcolare i residui standardizzati e lo facciamo attraverso le seguenti linee di codice:

```{r}
residuimultstandard <- residuimult/sd(residuimult)
residuimultstandard
```

Realizziamo quindi un grafico in cui i residui standardizzati vengono disegnati in funzione dei valori stimati
```{r}
plot(stimemult,residuimultstandard, 
     main="Residui standard rispetto ai valori stimati", 
     ylab="Residui standard", xlab="Valori Stimati",
     pch=5, col="red")
abline(h=0, col="blue", lty=2)
```

I punti risultano essere disposti anche in questo caso in maniera del tutto casuale e non è possibile notare nessuna tendenza.

### Coefficiente di determinazione

Come in precedenza, il coefficiente di determinazione è uguale a:

$$D^2 = \frac{\sum_{i=1}^n (\hat{y_i} - \bar{y})²}{\sum_{i=1}^n (y_i - \bar{y})²}$$

Ed esattamente come prima, se $D^2 = 0$ allora il modello di regressione multipla usato non spiega per nulla i dati, se $D^2 = 1$ allora il modello approssima perfettamente i dati.

Utilizziamo la funzione *summary(lm(y~$x_1+x_2+...+x_p$))$r.square* per calcolarlo. Utilizzando i dati a nostra disposizione otteniamo:

```{r}
summary(lm(lavoro ~ famiglia+studio+asilo+altro))$r.square
```

Essendo il valore del coefficiente decisamente vicino ad 1, il modello di regressione multipla utilizzato spiega in maniera ottima i dati.

## Regressione non lineare

Molto spesso la linearità non è un modello accettabile e lo notiamo quando il coefficiente di determinazione è prossimo allo 0.
Il modello lineare più semplice è il **modello di regressione polinomiale** che calcoliamo nella seguente maniera:

$$Y = \alpha + \beta X + \gamma X^2$$

Degli scatterplot creati in precedenza, in gran parte sembra che la linearità non basti. Esaminiamo nel dettagli la coppia "Lavoro" e "Asilo"

```{r}
plot(lavoro, asilo, main="Scatterplot", xlab="Lavoro", ylab="Asilo")
```

Calcolando il coefficiente di determinazione esso risulta pari a:

```{r}
summary(lm(asilo ~ lavoro))$r.square
```

Un valore che si trova nel mezzo ma che, utilizzando un modello di regressione non lineare possiamo far aumentare e approssimare meglio. 

Per la stima dei parametri $\alpha, \beta e \gamma$ è possibile ricorrere alla regressione multipla:

$$Y = \alpha + \beta X_1 + \gamma X_2$$

con regressori $X_1 = X$ e $X_2 = X^2$.

Utilizzando R possiamo stimare $\alpha, \beta$ e $\gamma$ con la funzione *lm(y~x + I(x^2))* dove *I()* è un *identificatore di variabile* e viene inserito quando vi è la necessità di effettuare operazione matematiche nelle variabili della regressione, in questo caso, l'elevazione a potenza.

Procediamo all'approssimazione polinomiale

```{r}
pol <- lm(asilo~lavoro+I(lavoro^2))
pol
```

```{r}

alphapol <- pol$coefficients[[1]]
betapol <- pol$coefficients[[2]]
gammapol <- pol$coefficients[[3]]

c(alphapol, betapol, gammapol)

```

Il modello polinomiale costruito ha quindi come valori $\alpha = 124.3816$, $\beta = -5.7236$ e come $\gamma = 0.0745$.
Procediamo col calcolo del coefficiente di determinazione attraverso la funzione *summary()*:

```{r}
summary(lm(asilo~lavoro+I(lavoro^2)))$r.square
```

Com'è possibile notare, il coefficiente di determinazione ottenuto col modello di regressione non lineare polinomia dal valore di 0.7709 è maggiore di quello di regressione lineare dal valore di 0.6252, risultando quindi nettamente migliore.

Andiamo adesso ad aggiungere la curva appena descritta allo scatterplot: 

```{r}
plot(lavoro, asilo, main="Scatterplot con curva", 
     xlab="Lavoro", ylab="Asilo", col = "red")
curve(alphapol+betapol*x+gammapol*x^2, add = TRUE)
```

# Analisi dei cluster

L'analisi dei cluster è una metodologia che permette di raggruppare in sottoinsiemi entità appartenenti ad un insieme più ampio. Questi sottoinsiemi sono chiamati **cluster**. Le diverse tecniche per effettuare l'analisi dei cluster hanno come unico obiettivo quello di ottenere raggruppamenti in base alla somiglianza in modo che gli elementi di uno stesso gruppo siano tra loro più simili possibili e gli elementi appartenenti a gruppi diversi siano più diversi possibili. 
Sia $n$ il numero di individui, il problema dell'analisi dei cluster sta nel determinare $m$ sottoinsiemi, i cluster, di individui in $I$, con $m$ minore intero di $n$, tali che $I_i$ appartenga soltanto ad un unico sottoinsieme.
Gli individui assegnati allo stesso cluster sono detti **simili**, mentre quelli assegnati a cluster diversi vengono chiamati **dissimili**.
Molte delle tecniche di analisi dei cluster prevedono la standardizzazione di ogni variabile usanto dal media campionaria e la deviazione standard campionaria, questo per far si che tutti i valori osservati siano privi di unità di misura. Questa operazione è detta *scalamento* e, oltre a rendere puri i valori osservati, ci permette di ridurne la grandezza. 
Preso un elemento $x_{ij}$, il corrispondente valore scalato è pari a:

$$ \frac{x_{ij} - \bar{x}}{s_j}$$
Nel nostro caso, i valori risultano essere in percentuali, quindi non vi è bisogno di effettuare scalamento, di seguito mostreremo solo gli effetti dello scalamento sulla nostra tabella.

R fornisce la funzione *scale()* che, data una matrice in input, permette di effettuare uno scalamento. Questa funzione prevede due attributi, *center* che di default è uguale a TRUE e indica se dagli elementi di ogni colonna della matrice si sottrae il valore medio della corrispondente colonna e *scale*, che di default è posto anch'esso a TRUE e indica se bisogna dividere gli elementi centrati di ogni colonna della matrice per la deviazione standard della rispettiva colonna.

Mostriamo i la nostra tabella:
```{r}
tabellaScalata <- scale(tabella)
tabellaScalata
```



## Distanza e similarità

Le misure metriche di somiglianza sono basate soprattutto sulle *funzioni distanza* tra i vettori delle caratteristiche. La definizione di questa funzione è la seguente:

1. $d(X_i, X_j) = 0$ se e solo se $ X_i = X_j$, con $X_i$ e $X_j$ in $E_p$;
2. $d(X_i, X_j) \geq  0$ per ogni $X_i$ e $X_j$ in $E_p$;
3. $d(X_i, X_j) = d(X_j, X_i) $ per ogni $X_i$ e $X_j$ in $E_p$;
4. $d(X_i, X_j) \leq d(X_i, X_k) + d(X_k, X_j)$ per ogni $X_i$ ,$X_j$ e $X_k$ in $E_p$.

Queste proprietà rappresentano le caratteristiche che deve possedere una funzione di distanza. Le distanze calcolate fra tutte le possibili coppie vengono inserite in una matrice simmetrica di cardinalità $n x n$, con ovviamente tutti 0 sulla diagonale, ed è quindi possibile considerare anche solo la matrice triangolare superiore.
R ha a disposizione la funzione *dist(X, method=, diag=FALSE, upper=FALSE)* per calcolare la matrice delle distanze secondo un metodo impostato dall'utente attraverso l'attributo *method*.

I possibili valori di method, rappresentano i metodi disponibili per il calcolo della matrice delle distanze e, in particolare abbiamo:

1. metrica euclidea (**euclidean**)
2. metrica del valore assoluto o metrica di Manhattan (**manhattan**)
3. metrica del massimo o metrica di Chebycev (**maximum**)
4. metrica di Minkowski (**minkowski**)
5. distanza di Canberra (**canberra**)
6. distanza di Jaccard (**binary**)

### Metrica euclidea

La metrica più diffusa e familiare è quella **euclidea**, definita come segue:

$$ d_{2}(X_i,X_j) = \sum_{k=1}^p [(x_{ik} - x_{jk})^2]^\frac{1}{2}$$

La metrica euclidea usata su tutti i dati, è fortemente influenzata dall'unità di misura base alla quale è valutata ciascuna delle p caratteristiche. 
Per usare questa metrica in R basta settare *"euclidean"* come valore dell'attributo *method* della funzione *dist()*.

Per i dati a nostra disposizione abbiamo che:

```{r}
dist(tabella, method="euclidean", diag=FALSE, upper=FALSE)
```

\newpage
### Metrica di Manhattan

La metrica di Manhattan è così definita:

$$d_1(X_i, X_j) = \sum_{k=1}^p |x_{ik} - x_{jk}| $$

Calcoleremo queste altre metriche utilizzate solo su una parte delle regioni, giusto per mostrare la differenza nei risultati.

Per utilizzare questa metrica, bisogna dare *"manhattan"* come valore all'attributo *method* della funzione *dist()*.

```{r}
x <- tabella[1:5, 1:5]
x
```

```{r}
dist(x, method = "manhattan", diag = FALSE, upper=FALSE)
```

### Metrica di Chebycev o del massimo

La metrica di Chebycev è definita nella seguente maniera:

$$d_\infty (X_i, X_j) = max_{k=1,...,p}|X_{ik} - X_{jk}| $$

Per utilizzare questa metrica, bisogna dare *"maximum"* come valore all'attributo *method* della funzione *dist()*. 

```{r}
dist(x, method = "maximum", diag = FALSE, upper=FALSE)
```

### Metrica di Minkowski

La metrica di Minkowsi è definita come segue:

$$d_r(X_i, X_j) = [\sum_{k=1}^p|x_{ik}-x_{jk}|^r]^\frac{1}{r}$$

con $r \geq 1$. Se $r=2$ si ottiene la metrica euclidea, se invece $r=1$ allora si ottiene la metrica del valore assoluto mentre se $r=\infty$ si ottiene la metrica di Chebycev.
Per utilizzare questa metrica, bisogna dare *"minkowski"* come valore all'attributo *method* della funzione *dist()* e bisogna fornire il parametro r.
Nell'esempio seguente abbiamo posto $r=4$

```{r}
dist(x, method = "minkowski", 4, diag = FALSE, upper=FALSE)
```

### Metrica di Canberra

La metrica di Canberra è definita come segue:

$$d_c(X_i,X_j) = \sum_{k=1}^p\frac{|x_{ik} - x_{jk}|}{|x_{ik}+x_{jk}|}$$
Per utilizzare questa metrica, bisogna dare *"canberra"* come valore all'attributo *method* della funzione *dist()*. 

```{r}
dist(x, method = "canberra", diag = FALSE, upper=FALSE)
```

### Metrica di Jaccard

In R è disponibile la distanza di Jaccard solo per vettori binari, diversi da quelli a nostra disposizione. Non è quindi possibile calcolarla.

## Misure di similarità 
In alternativa alla matrice delle distanze, è possibile utilizzare una matrice delle similarità.
Una **misura di similarità** fornisce un valore numerico compreso tra 0 e 1 e permette di definire quantitativamente la somiglianza o differenza fra due individui.
In generale, una funzione a valori reali $s_{ij} = s(X_i, X_j)$ è detta misura di similarità se e soltanto se essa soddisfa le seguenti condizioni:

1.  $s(X_i,X_i) = 1$;
2.  $0\leq s(X_i,X_j) \leq 1$;
3.  $s(X_i,X_j) = s(X_j,X_i)$ per ogni $X_i$ e $X_j$

La quantità $s_{ij}$ è detta **coefficiente di similarità**. La principale differenza fra una misura di distanza e una di similarità è che la prima è sempre maggiore o uguale di 0 mentre la seconda risulta essere compresa fra -1 e 1. È sempre possibile trasformare una misura di distanza in una misura di similarità nella seguente maniera:

$$s_{ij} = \frac{1}{1+d_{ij}} \qquad (i,j = 1, 2,..., n)$$

## Misura di non omogeneità totale

Consideriamo un insieme di n individui e supponiamo esista un insieme di p caratteristiche osservabili e possedute da ogni individuo. Alla matrice delle misure è possibile associare una matrice $W_x$ di cardinalità $p*p$ detta **matrice delle varianze e covarianze** dove il generico elemento $w_{rl}$ è definito nella seguente maniera:

$$w_{rl} = \frac{1}{n-1}\sum_{i=1}^n (x_{ir} - \bar{x_{r}})(x_{il} - \bar{x_{l}}) \qquad (r,l= 1,2,...,p)$$

Se $r=l$ allora l'elemento $w_{rl}$ sarà uguale alla varianza campionaria relativa alla caratteristica r-esima effettuata su tutti gli n individui, mentre se $r \neq l$ l'elemento $w_{rl}$ è uguale alla covarianza campionaria tra la caratteristica r-esima e la caratteristica l-esima effettuata su tutti gli n individui.

Dalla matrice delle varianze e covarianze è possibile ricavare una matrice chiamata **matrice statistica di non omogeneità**, indicata con $H_I$, per l'insieme I di individui di cardinalità $p*p$ dove il generico elemento è definito nella seguente maniera:

$$ h_{rl} = \sum_{i=1}^{n} (x_{ir} - \bar{x_r}) (x_{il} - \bar{x_l}) = (n-1)w_{rl} \qquad (r,l = 1,2,...,p)$$

In questa matrice, quando $r=l$ l'elemento $h_{rr}$ corrisponde a n-1 volte la varianza campionaria della caratteristica r-esima su tutti gli individui.
Una **misura di non omogeneità statistica ** dell'insieme I di individui corrisponde alla **traccia** delle matrice $H_I$ ed è definita come segue:

$$ trH_I = \sum_{r=1}^{p}h_{rr} = (n-1)\sum_{r=1}^{p}s_r^2$$
oppure in alternativa:

$$trH_I = \sum_{i=1}^{n}d_2^2(X_i, \bar{X})$$

dove $d_2$ indica la distanza euclidea e $\bar{X} = (\bar{x_1}, \bar{x_2}, ..., \bar{x_p})$ è un vettore dove il generico elemento $\bar{x_j}$ rappresenta la media campionaria della caratteristca j-esima sugli n individui.

## Misure di non omogeneità tra cluster

Oltre a misure di non omogeneità relative all'insieme totale, vanno definite anche misure di non omogeneità all'interno dei cluster e tra cluster distinti. Infatti, al termine della classificazione, gli individui che appartengono allo stesso cluster dovrebbero essere il più omogenei tra loro e il più differenti da quelli appartenenti a cluster diversi.

È quindi possibile definire una **misura di non omogeneità totale** che tenga conto della non omogeneità interna ai cluster (*within*) e della non omogeneità fra cluster (*between*). Denotiamo questa misura di non omogeneità totale con T e la definiamo come:

$$T = S + B$$

dove *S* corrisponde alla somma delle matrici di non omogeneità statistica relative ai singoli cluster ed *B* la matrice di non omogeneità statistica tra i vari cluster considerati.
Le matrici T, S e B risultano avere tutte e tre la stessa cardinalità. Per ogni fissata matrice di dati, anche la matrice T è fissata mentre invece, le matrici S e B dipendono strettamente dalla partizione in cluster considerata. 
Per ogni partizione dell'insieme degli individui in un numero fissato di cluster, otteniamo che:

$$trT = trS + trB$$

o, in maniera equivalente:

$$1 = \frac{trS}{trT} + \frac{trB}{trT}$$
e siccome abbiamo detto che *trT* è fissata allora i cluster dovrebbero essere individuati in modo da **minimizzare** *la misura di non omogeneità statistica all'interno dei cluster* e **massimizzare** *la misura di non omogeneità statistica fra cluster*.

## Metodi di ottimizzazione
Scelta la misura di distanza (o la similarità in alternativa) bisogna scegliere un algoritmo di raggruppamento delle unità osservate. 
I metodi di raggruppamento si suddividono in tre tipi:

* **Metodo di enumerazione completa**;
* **Metodi gerarchici**;
* **Metodi non gerarchici**.

Il primo metodo elencato, quello di enumerazione completa (di ottimizzazione), è computazionalmente oneroso da applicare in quanto bisogna calcolare la funzione obiettivo per ogni partizione dell'insieme totale e questo va bene solo quando abbiamo un insieme di dati piccolo. Vengono per questo adottati metodi di raggruppamento gerarchici e non gerarchici che operano su sottoclassi delle partizioni degli individui in cluster.

### Metodi non gerarchici

Si è scelto di effettuare prima la suddivisione in cluster utilizzando dei metodi non gerarchici in modo da stabilire il numero di partizioni ottimale, sfruttandolo successivamente nell'analisi mediante metodi gerarchici.
I metodi non gerarchici consentono di riallocare gli individui già classificati precedentemente nell'analisi e l'obiettivo finale di queste tecniche è quello di ottenere un'unica partizione degli individui di partenza. Per quanto riguarda il numero di cluster da individuare, in molti metodi non gerarchici si assume che esso sia fissato da chi sta facendo l'analisi mentre in altri questo numero è determinato durante l'analisi stessa.
Il metodo non gerarchico più utilizzato è quello del **k-means** (Hartigan e Wong, 1979) nel quale il numero di cluster è specificato a priori e fornisce in output un'unica partizione.
I passi dell'algoritmo sono i seguenti:

1. Fissare il numero *m* dei cluster specificando *m* punti iniziali (scegliendo alcuni individui o prendendo una configurazione determinata con una tecnica gerarchica) che inducono una prima partizione provvisoria;

2. Considerare tutti gli individui e attribuire ciascuno di essi al cluster individuato dal punto di riferimento da cui ha distanza minore;

3. Calcolare il baricentro (centroide) di ognuno degli m gruppi così ottenuti. Tali centroidi costituiscono i punti di riferimento per i nuovi cluster;

4. Valutare ogni distanza di ogni unità dal centroide ottenuto al punto precedente;

5. Ricalcolare i centroidi dei k gruppi così ottenuti;

6. Ripeto il procedimento dal punto 4 fino ad ottenere una partzione che risulta essere stabile oppure dopo aver raggiunto un determinato numero di iterazioni.

Per garantire la convergenza della procedura iterativa, viene utilizzata la distanza euclidea come misura di distanza fra i vettori delle caratteristiche e i centroidi e viene considerata *la matrice contenente i quadrati delle distanze euclidee*.
Il metodo k-means risulta essere molto veloce nei calcoli e lascia estrema libertà agli individui di allontanarsi o raggrupparsi. Uno svantaggio riguarda il fatto che la classificazione finale può essere influenzata dalla scelta iniziale degli m vettori delle caratteristiche come punti di riferimento. Infatti vi è il rischio di cadere in un ottimo locale e non in uno globale.

Per utilizzare il metodo k-means in R bisogna utilizzare la funzione *kmeans(X, centers, iter.max=N, nstart=M)*. Per la nostra analisi utilizzeremo un valore di centers inizialmente pari a 2, un numero di iterazioni pari a 25 e un valore di nstart uguale a 10.

```{r}
km <- kmeans(tabella, centers = 2, iter.max = 25, nstart = 10)
km
```

La suddivisione in cluster individuata è la seguente:

- $C_1 = {Piemonte, Valle d'Aosta, Liguria, Lombardia, Trentino-Alto Adige, Veneto, Friuli-Venezia Giulia, Emilia Romagna, Toscana, Umbria, Marche, Lazio, Abruzzo, Campania}$
- $C_2 = {Molise, Puglia, Basilicata, Calabria, Sicilia, Sardegna}$


```{r}
km$totss
```

```{r}
km$tot.withinss
```

```{r}
km$betweenss
```

```{r}
km$betweenss/km$totss
```

Possiamo notare che la misura di non omogeneità totale è pari a 7067.801, mentre la somma delle misure di non omogeneità totale interne è uguale a 2540.527, la misura di non omogeneità tra cluster diversi è 4527.274 e il rapporto fra $\frac{trB}{trT}$ risulta essere pari a 0.6405, un valore non sufficiente. Proviamo quindi ad aumentare il numero di centri a 3.

```{r}
km <- kmeans(tabella, centers = 3, iter.max = 25, nstart = 10)
km
```

La suddivisione in cluster individuata è la seguente:

- $C_1 = {Piemonte, Valle d'Aosta, Liguria, Lombardia, Trentino-Alto Adige, Veneto, Friuli-Venezia Giulia, Emilia Romagna, Toscana, Umbria, Marche, Lazio, Abruzzo, Campania}$
- $C_2 = {Molise}$
- $C_3 = {Puglia, Basilicata, Calabria, Sicilia, Sardegna}$

```{r}
km$totss
```

```{r}
km$tot.withinss
```

```{r}
km$betweenss
```

```{r}
km$betweenss/km$totss
```

Possiamo notare che la misura di non omogeneità totale è pari a 7067.801, mentre la somma delle misure di non omogeneità totale interne è uguale a 1612.28, la misura di non omogeneità tra cluster diversi è 5455.51 e il rapporto fra $\frac{trB}{trT}$ risulta essere pari a 0.771, un valore più che sufficiente considerando il numero di centri utilizzato.

Nell'analisi che effettueremo con i metodi gerarchici utilizzeremo lo stesso numero di cluster.

È possibile anche scegliere i centroidi come punti di riferimento:

```{r}
d<-dist(tabella, method="euclidean", diag = TRUE, upper = TRUE)
d2 <- d^2
tree <- hclust(d2, method = "centroid")
taglio <- cutree(tree, k = 3, h = NULL)
tagliolist <- list(taglio)
centroidiIniziali <- aggregate(tabella, tagliolist, mean)[,-1]
kmeans(tabella, centers = centroidiIniziali, iter.max = 25)
```

Com'è possibile vedere anche utilizzando i centroidi abbiamo esattamente la stessa suddivisione in cluster presentata precedentemente.

### Metodi gerarchici

I metodi gerarchici si suddividono in: **agglomerativi** e **divisivi**. In quelli di tipo agglomerativo si parte da una situazione in cui si hanno tanti cluster quanti gli individui per poi giungere ad un unico cluster dove sono riuniti tutti. Nei metodi gerarchici di tipo divisivo invece, si parte da una situazione in cui si ha un solo cluster con tutti gli individui per poi raggiungere, attraverso divisioni, una situazione in cui si hanno tanti cluster quanti sono gli individui.
L'obiettivo finale dei metodi gerarchici è quello di ottenere una sequenza di partizioni che possono essere rappresentate graficamente mediante una struttura chiamata **dendrogramma**, sul quale asse delle ordinate sono riportati i livelli di distanza e su quello delle ascisse i singoli elementi. Ad ogni livello corrisponde una partizione.

In molti metodi gerarchici di tipo agglomerativo l'algoritmo ha una struttura comune:

1. A partire dalla matrice dei dati, dobbiamo costruire la matrice delle distanze D (oppure quella delle similiarità S) tra individui considerati come singoli cluster distinti.

2. Individuare la coppia di cluster meno distanti (o più somiglianti) e raggruppare in un unico cluster i due cluster meno distanti (o più somiglianti); calcolare inoltre la distanza (o similarità) di questo cluster originato dall'agglomerazione da tutti gli altri gruppi già esistenti.

3. Costruire una nuova matrice di distanza (o similarità) che risulterà ridotta di una riga e di una colonna rispetto a quella che la precede (questo perchè le due righe e le due colonne dei singoli cluster sono state agglomerate in una singola rigoa e una singola colonna con le nuove distanze).

4. Operare sulla nuova matrice a partire dallo step 2 fino ad esaurire tutte le possibilità di raggruppamento (fino a quando non si ottiene una matrice 2x2).

5. Rappresentare graficamente il processo di agglomerazione attraverso un dendrogramma.

Quello che cambia nei diversi metodi è il passo 1 e il passo 2. Nel primo bisogna scegliere fra la similarità o la distanza mentre le tecniche adottate nel secondo passo sono quelle che differenzieranno di molto i vari metodi e da cui dipende il nome del metodo stesso.

L'analisi gerarchica di tipo agglomerativo viene effettuata in R attraverso la funzione *hclust(d, method =)* dove:

- **d** è rappresenta un oggetto creato tramite la funzione *dist()*;
- **method** seleziona il metodo gerarchico agglomerativo. Il valore di default è *complete*.

Abbiamo 5 diversi metodi fra cui scegliere:

1. Metodo del legame singolo (*single*);

2. Metodo del legame completo (*complete*);

3. Metodo del legame medio (*average*);

4. Metodo del centroide (*centroid*);

5. Metodo della mediana (*median*)

#### Metodo del legame singolo

In questo metodo la distanza fra due gruppi $G_1$ e $G_2$ contenenti rispettivamente $n_1$ e $n_2$ individui è definita come la minima fra tutte le distanze che si possono calcolare tra ogni individuo di $G_1$ e ogni individuo di $G_2$. Vengono poi agglomerate i gruppi con distanza minore come descritto precedentemente.
Il metodo del legame singolo ha come vantaggio quello di consentire di individuare gruppi di qualsiasi forma.
Attraverso R adesso procediamo all'analisi dei cluster attraverso questo metodo:

```{r}
d <- dist(tabella, method = "euclidean", diag = TRUE, upper = FALSE)
hls <- hclust(d, method = "single")
str(hls)
```

Procediamo adesso alla stampa del dendrogramma:

```{r}
plot(hls, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame singolo")
axis(side = 4, at= round(c(0, hls$height), 2))
```

La funzione *axis(side = 4, at= round(c(0, hls$height), 2))* ci permette di costruire l'asse delle altezze alla destra del grafico, arrotondando alla seconda cifra decimale.

Per scegliere una buona partizione del dendrogramma, consideriamo un grafico detto **screeplot**, sul quale asse delle ordinate vengono posti i numeri di gruppi ottenibili con il meotodo gerarchico e sull'asse ascisse vengono le distanze a cui avvengono le successive aggregazioni fra gruppi. 
Al fine di fornire misure quantitative degli incrementi riscontrati tra le distanze a cui avvengono le successive agglomerazioni, consideriamo la seguente quantità:
$$\delta_k = d_{k-1} - d_k \qquad (k=2,...,n)$$
dove $d_k$ rappresenta il livello di distanza a cui è stata effettuata l'agglomerazione in k gruppi e n è il numero iniziale di individui.
Quando $\delta_k$ risulta sufficientemente elevato allora i gruppi sono dissimili fra loro ed è possibile tagliare il dendrogramma all'altezza corrispondente alla partizione in k gruppi.
È possibile utilizzare lo screeplot solo per i metodi gerarchici del legame singolo, medio e completo in quanto con i metodi del centroide e della mediana, le successive agglomerazioni potrebbero verificarsi ad un livello di distanza minore o uguale rispetto alle precedenti. Questa procedura però non fornisce sempre la suddivisione in cluster più adeguata, infatti è sempre preferibile utilizzare le misure di non omogeneità statistiche.

```{r}
plot(c(0, hls$height), seq(20, 1), type = "b", main = "screeplot", 
     xlab = "Distanza di aggregazione", ylab = "Numero di cluster", col = "red")
```

```{r}
hls$height
```

In questo caso il max lo abbiamo fra:

$$\delta_2 = h_1 - h_2 = 24.909719 - 14.428444 = 10.48127 $$
Quindi il valore di k per il quale $\delta_k$ è massimo si ha con $k=2$. Applichiamo la suddivisione al dendrogramma utilizzando la funzione *rect.hclust()*:
```{r}
plot(hls, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame singolo")
axis(side = 4, at= round(c(0, hls$height), 2))
rect.hclust(hls, k=2)
```

Per ottenere la suddivisione in cluster oltre che graficamente, è possibile utilizzare la funzione *cutree()*.

```{r}
cutree(hls, k=2)
```

Calcoliamo adesso la misura di non omogeneità statistica totale

```{r}
n <- nrow(tabella)
trHI <- (n-1) * sum(apply(tabella, 2, var))
trHI
```

La misura di non omogeneità statistica totale è pari a 7097.801. Procediamo adesso al calcolo delle misure di non omogeneità statistiche e lo facciamo per la suddivisione in 2,3,4 cluster.

```{r}
d <- dist(tabella, method="euclidean", diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "single")
taglio <-cutree(tree, k=2, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trS <- trH1 + trH2

trB <- trHI-trH1-trH2
trB
trB/trHI
```

Con la suddivisione in due cluster otteniamo una misura di non omogeneità tra cluster pari a 2681.522. Questa suddivisione ricordiamo, ci è stata suggerita anche dallo screeplot. Il rapporto fra trB/trHI è pari a 0.3793997 un valore decisamente basso che ci porta all'aumentare il numero di cluster.

```{r}
d <- dist(tabella, method="euclidean", diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "single")
taglio <-cutree(tree, k=3, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trS <- trH1 + trH2 + trH3 

trB <- trHI-trH1-trH2-trH3
trB/trHI
```

Con la suddivisione in tre cluster, suggeritaci durante l'analisi con i metodi non gerarchici, il rapporto fra trB/trHI è pari a 0.4232935 un valore che, seppur più alto di quello a due cluster risulta comunque essere basso.

```{r}
d <- dist(tabella, method="euclidean", diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "single")
taglio <-cutree(tree, k=4, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trH4<-(num[[4]]-1) * sum(agvar[4,])
if(is.na(trH4))
  trH4 <- 0

trS <- trH1 + trH2 + trH3 + trH4

trB <- trHI-trH1-trH2-trH3-trH4
trB/trHI
```

La suddivisione in quattro cluster ci ha portato al risultato migliore fra le tre suddivisioni con un rapporto fra trB/trHI pari a 0.7699938, che ricade nel range di valori accettabili.

```{r}
plot(tree, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame singolo")
axis(side = 4, at= round(c(0, tree$height), 2))
rect.hclust(tree, k=4)
```


### Metodo del legame completo

In questo metodo la distanza tra i gruppi $G_1$ e $G_2$ è definita come la massima tra tutte le $n_1n_2$ distanze che si possono calcolare tra ogni individuo di $G_1$ e ogni individuo di $G_2$.
A livello 0, viene considerato inizialmente un insieme di *n* cluster, uno per ogni individuo.
Nel passo successivo, si cerca il coefficiente di distanza minima all'interno della matrice delle distanze D e si raggruppano nello stesso cluster i due individui ad esso associati.
A livello 1 si modifica la matrice delle distanze valutando le distanze dal cluster appena creato con gli altri mediante la seguente relazione:

$$d_{(ij),k} = max(d_{ik}, d_{jk})$$
Ad ogni passo successivo vengono ripetute le stesse azioni, funo ad ottenere un unico cluster formato da tutti gli individui.

```{r}
d <- dist(tabella, method = "euclidean", 
          diag = TRUE, upper = FALSE)
hlc <- hclust(d, method = "complete")
str(hlc)
```

Procediamo adesso alla stampa del dendrogramma:

```{r}
plot(hlc, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame completo")
axis(side = 4, at= round(c(0, hlc$height), 2))
```

Come fatto in precedenza, procediamo alla stampa dello screeplot:
```{r}
plot(c(0, hlc$height), seq(20, 1), type = "b",
     main = "screeplot", xlab = "Distanza di aggregazione", 
     ylab = "Numero di cluster", col = "red")
```

```{r}
hlc$height
```

Anche in questo caso il max lo abbiamo fra:

$$\delta_2 = h_1 - h_2 = 70.274764 - 43.088240 = 27.186524 $$
Quindi il valore di k per il quale $\delta_k$ è massimo si ha con $k=2$. Applichiamo la suddivisione al dendrogramma utilizzando la funzione *rect.hclust()*:
```{r}
plot(hlc, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame completo")
axis(side = 4, at= round(c(0, hlc$height), 2))
rect.hclust(hlc, k=2)
```

Per ottenere la suddivisione in cluster oltre che graficamente, è possibile utilizzare la funzione *cutree()*.

```{r}
cutree(hlc, k=2)
```

Procediamo adesso all'analisi utilizzando le misure di non omogeneità statistiche, in quanto, anche in precedenza sono risultate essere più precise rispetto allo screeplot.
Faremo l'analisi per la suddivisione in 2,3 e 4 cluster.

Siccome la misura di non omogeneità statistica totale è uguale a quella calcolata in precedenza evitiamo di ricalcolarla.

Procediamo al calcolo delle misure di omogeneità statistiche per le varie suddivisioni:

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "complete")
taglio <-cutree(tree, k=2, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trS <- trH1 + trH2 

trB <- trHI-trH1-trH2
trB/trHI
```

Con la suddivisione in due cluster, suggeritaci dallo screeplot, il rapporto tra $\frac{trB}{trT}$ non è risultato essere abbastanza alto.

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "complete")
taglio <-cutree(tree, k=3, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trS <- trH1 + trH2 + trH3 

trB <- trHI-trH1-trH2-trH3
trB/trHI
```

Con la suddivisione in tre cluster, suggeritaci durante l'analisi con i metodi non gerarchici, invece il rapporto tra $\frac{trB}{trT}$ ha raggiunto un risultato di sufficienza ed è possibile tenerlo in considerazione.  Da notare che il valore è pressochè identico a quello trovato con il metodo k-means. 

```{r}
plot(hlc, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame completo")
axis(side = 4, at= round(c(0, hlc$height), 2))
rect.hclust(hlc, k=3)
```

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "complete")
taglio <-cutree(tree, k=4, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trH4<-(num[[4]]-1) * sum(agvar[4,])
if(is.na(trH4))
  trH4 <- 0

trS <- trH1 + trH2 + trH3 + trH4

trB <- trHI-trH1-trH2-trH3-trH4
trB/trHI
```

Con quest'ultima suddivisione, il rapporto fra $\frac{trB}{trT}$ raggiunge il risultato più alto fra tutti quelli visti con i metodi gerarchici e risulta essere un ottimo risultato dato il numero di cluster scelti.

La relativa suddivisione in cluster è la seguente:

```{r}
cutree(hlc, k=4, h=NULL)
```

$C_1$ *= {Campania, Lazio, Toscana}*
$C_2$ *= {Emilia Romagna, Umbria, Valle d'Aosta, Veneto, Lombardia, Trentino-alto Adige, Friuli-venezia Giulia, Liguria, Marche, Piemonte, Abruzzo}*
$C_3$ *= {Molise}*
$C_4$ *= {Sicilia, Sardegna, Basilicata, Puglia, Calabria}*

```{r}
plot(hlc, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame completo")
axis(side = 4, at= round(c(0, hlc$height), 2))
rect.hclust(hlc, k=4)
```


### Metodo del legame medio

In questo metodo la distanza tra i gruppi $G_1$ e $G_2$ è definita come la media aritmetica delle distanze fra tutte le coppie che compongono gli insiemi.
All'inizio del metodo, a livello 0 abbiato tanti cluster quanti sono gli individui. Successivamente controlliamo la matrice delle distanze D e raggruppiamo nello stesso cluster gli individui associati alla distanza minima. 
A livello 1 di modifica la matrice delle distanze valutando le distanze del cluster appena creato e tutti gli altri rimanenti, mediante la seguente relazione:

$$d_{(i,j),k} = \frac{1}{2}(d_{i,k}+d_{j,k}) \qquad (k=1,2,...,n;k\neq i,j)$$
Viene quindi costruita una nuova matrice delle distanze. Ad ogni passo successivo, dopo aver individuato i due cluster da raggruppare, la distanza fra il nuovo cluster $G_{uv}$ e un altro cluster $G_z$ è definita nella seguente maniera:

$$d_{(uv),z} = \frac{N_u}{N_u+N_v}d_{uz} + \frac{N_v}{N_u+N_v}d_{vz}$$

dove $N_u$, $N_v$ e $N_z$ sono rispettivamente il numero di individui del cluster $G_u$, $G_v$ e $G_z$.
La procedura si ripete fino a quando non si ottiene un unico cluster con tutti gli individui.

```{r}
d <- dist(tabella, method = "euclidean", 
          diag = TRUE, upper = FALSE)
hlm <- hclust(d, method = "average")
str(hlm)
```

Procediamo adesso alla stampa del dendrogramma:

```{r}
plot(hlm, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame medio")
axis(side = 4, at= round(c(0, hlm$height), 2))
```

Come fatto in precedenza, procediamo alla stampa dello screeplot:
```{r}
plot(c(0, hlm$height), seq(20, 1), type = "b", 
     main = "screeplot", xlab = "Distanza di aggregazione", 
     ylab = "Numero di cluster", col = "red")
```

```{r}
hlm$height
```

Anche in questo caso il max lo abbiamo fra:

$$\delta_2 = h_1 - h_2 = 53.603492 - 28.947554 = 24.65594 $$

Quindi il valore di k per il quale $\delta_k$ è massimo si ha con $k=2$. Applichiamo la suddivisione al dendrogramma utilizzando la funzione *rect.hclust()*:
```{r}
plot(hlm, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame medio")
axis(side = 4, at= round(c(0, hlm$height), 2))
rect.hclust(hlm, k=2)
```

Per ottenere la suddivisione in cluster oltre che graficamente, è possibile utilizzare la funzione *cutree()*.

```{r}
cutree(hlm, k=2)
```

Procediamo adesso all'analisi utilizzando le misure di non omogeneità statistiche, in quanto, anche in precedenza sono risultate essere più precise rispetto allo screeplot.
Faremo l'analisi per la suddivisione in 2,3 e 4 cluster.

Siccome la misura di non omogeneità statistica totale è uguale a quella calcolata in precedenza evitiamo di ricalcolarla.

Procediamo al calcolo delle misure di omogeneità statistiche per le varie suddivisioni:

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "average")
taglio <-cutree(tree, k=2, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trS <- trH1 + trH2 

trB <- trHI-trH1-trH2
trB/trHI
```

Con la suddivisione in due cluster, suggeritaci dallo screeplot, il rapporto tra $\frac{trB}{trT}$ è risultato essere estremamente basso.

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "average")
taglio <-cutree(tree, k=3, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trS <- trH1 + trH2 + trH3 

trB <- trHI-trH1-trH2-trH3
trB/trHI
```

Con la suddivisione in tre cluster, suggeritaci durante l'analisi con i metodi non gerarchici, invece il rapporto tra $\frac{trB}{trT}$ ha raggiunto un risultato di sufficienza ed è possibile tenerlo in considerazione. Da notare che il valore è pressochè identico a quello trovato con il metodo k-means. 

```{r}
plot(hlm, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame medio")
axis(side = 4, at= round(c(0, hlm$height), 2))
rect.hclust(hlm, k=3)
```

```{r}
d <- dist(tabella, method="euclidean",
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "average")
taglio <-cutree(tree, k=4, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trH4<-(num[[4]]-1) * sum(agvar[4,])
if(is.na(trH4))
  trH4 <- 0

trS <- trH1 + trH2 + trH3 + trH4

trB <- trHI-trH1-trH2-trH3-trH4
trB/trHI
```

Con quest'ultima suddivisione, suggeritaci dal metodo k-means fatto in precedenza, il rapporto fra $\frac{trB}{trT}$ raggiunge il risultato più alto fra i precedenti ed è in linea con quello ottenuto con il metodo del legame completo e risulta essere un ottimo risultato dato il numero di cluster scelti. 

La relativa suddivisione in cluster è la seguente:

```{r}
cutree(hlm, k=4, h=NULL)
```

$C_1$ *= {Campania}*
$C_2$ *= {Emilia Romagna, Umbria, Valle d'Aosta, Veneto, Lombardia, Trentino-alto Adige, Friuli-venezia Giulia, Liguria, Marche, Piemonte, Abruzzo, Lazio, Toscana}*
$C_3$ *= {Molise}*
$C_4$ *= {Sicilia, Sardegna, Basilicata, Puglia, Calabria}*

```{r}
plot(hlm, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del legame medio")
axis(side = 4, at= round(c(0, hlm$height), 2))
rect.hclust(hlm, k=4)
```

### Metodo del centroide

In questo metodo la distanza tra il gruppo $G_1$ e il gruppo $G_2$ è definita come la distanza tra i centroidi, ovvero le medie campionarie calcolate sugli individui dei gruppi.
A differenza dei metodi precedenti qui viene considerata la matrice contenente i quadrati delle singole distanze euclidee.

```{r}
d <- dist(tabella, method = "euclidean", 
          diag = TRUE, upper = FALSE)
d2 <- d^2
hlcentroid <- hclust(d2, method = "centroid")
str(hlcentroid)
```

Procediamo adesso alla stampa del dendrogramma:

```{r}
plot(hlcentroid, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del centroide")
axis(side = 4, at= round(c(0, hlcentroid$height), 2))
```

Procediamo adesso all'analisi utilizzando le misure di non omogeneità statistiche.
Faremo l'analisi per la suddivisione in 2,3 e 4 cluster.

Siccome la misura di non omogeneità statistica totale è uguale a quella calcolata in precedenza evitiamo di ricalcolarla.

Procediamo al calcolo delle misure di omogeneità statistiche per le varie suddivisioni:

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "centroid")
taglio <-cutree(tree, k=2, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trS <- trH1 + trH2 

trB <- trHI-trH1-trH2
trB/trHI
```

Con la suddivisione in due cluster, suggeritaci dallo scatterplot, il rapporto tra $\frac{trB}{trT}$ è risultato essere estremamente basso.

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "centroid")
taglio <-cutree(tree, k=3, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trS <- trH1 + trH2 + trH3 

trB <- trHI-trH1-trH2-trH3
trB/trHI
```

Con la suddivisione in tre cluster, suggeritaci durante l'analisi con i metodi non gerarchici, invece il rapporto tra $\frac{trB}{trT}$ ha raggiunto un risultato di sufficienza ed è possibile tenerlo in considerazione.

```{r}
plot(hlcentroid, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del centroide")
axis(side = 4, at= round(c(0, hlcentroid$height), 2))
rect.hclust(hlcentroid, k=3)
```

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "centroid")
taglio <-cutree(tree, k=4, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trH4<-(num[[4]]-1) * sum(agvar[4,])
if(is.na(trH4))
  trH4 <- 0

trS <- trH1 + trH2 + trH3 + trH4

trB <- trHI-trH1-trH2-trH3-trH4
trB/trHI
```

Con quest'ultima suddivisione, il rapporto fra $\frac{trB}{trT}$ raggiunge il risultato più alto fra i precedenti ed è poco inferiore a quello ottenuto con i due metodi precedenti e risulta essere un ottimo risultato dato il numero di cluster scelti.

La relativa suddivisione in cluster è la seguente:

```{r}
cutree(hlm, k=4, h=NULL)
```

$C_1$ *= {Emilia Romagna, Umbria, Valle d'Aosta, Veneto, Lombardia, Trentino-alto Adige, Friuli-venezia Giulia, Liguria, Marche, Piemonte, Abruzzo, Lazio, Toscana}*
$C_2$ *= {Molise}*
$C_3$ *= {Campania}*
$C_4$ *= {Sicilia, Sardegna, Basilicata, Puglia, Calabria}*

```{r}
plot(hlcentroid, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="del centroide")
axis(side = 4, at= round(c(0, hlcentroid$height), 2))
rect.hclust(hlcentroid, k=4)
```

### Metodo della mediana

Questo metodo è simile a quello del centroide e differisce per il fatto che la procedura è indipendente dalla numerosità dei cluster. Infatti, quando due gruppi si aggregano il nuovo centroide è calcolato come la semisomma dei due centroidi precedenti.
Anche il metodo della mediana, come quello del legame singolo può dare vita alla formazione di una catena fra gli individui.

```{r}
d <- dist(tabella, method = "euclidean", 
          diag = TRUE, upper = FALSE)
d2 <- d^2
hlmedian <- hclust(d2, method = "median")
str(hlmedian)
```

Procediamo adesso alla stampa del dendrogramma:

```{r}
plot(hlmedian, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="della mediana")
axis(side = 4, at= round(c(0, hlmedian$height), 2))
```

Procediamo adesso all'analisi utilizzando le misure di non omogeneità statistiche.
Faremo l'analisi per la suddivisione in 2,3 e 4 cluster.

Siccome la misura di non omogeneità statistica totale è uguale a quella calcolata in precedenza evitiamo di ricalcolarla.

Procediamo al calcolo delle misure di omogeneità statistiche per le varie suddivisioni:

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "median")
taglio <-cutree(tree, k=2, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trS <- trH1 + trH2 

trB <- trHI-trH1-trH2
trB/trHI
```

Con la suddivisione in due cluster, suggeritaci dallo scatterplot, il rapporto tra $\frac{trB}{trT}$ è risultato essere estremamente basso.

```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "median")
taglio <-cutree(tree, k=3, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trS <- trH1 + trH2 + trH3 

trB <- trHI-trH1-trH2-trH3
trB/trHI
```

Con la suddivisione in tre cluster, suggeritaci durante l'analisi con i metodi non gerarchici, invece il rapporto tra $\frac{trB}{trT}$ ha raggiunto un risultato di sufficienza ed è possibile tenerlo in considerazione.
```{r}
plot(hlmedian, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="della mediana")
axis(side = 4, at= round(c(0, hlmedian$height), 2))
rect.hclust(hlmedian, k=3)
```


```{r}
d <- dist(tabella, method="euclidean", 
          diag = TRUE, upper=TRUE)
d2<-d^2
tree <- hclust(d2, method = "median")
taglio <-cutree(tree, k=4, h=NULL)
num <- table(taglio)
tagliolist <- list(taglio)

agvar <- aggregate(tabella, tagliolist, var)[,-1]

trH1<-(num[[1]]-1) * sum(agvar[1,])
if(is.na(trH1))
  trH1 <- 0

trH2<-(num[[2]]-1) * sum(agvar[2,])
if(is.na(trH2))
  trH2 <- 0

trH3<-(num[[3]]-1) * sum(agvar[3,])
if(is.na(trH3))
  trH3 <- 0

trH4<-(num[[4]]-1) * sum(agvar[4,])
if(is.na(trH4))
  trH4 <- 0

trS <- trH1 + trH2 + trH3 + trH4

trB <- trHI-trH1-trH2-trH3-trH4
trB/trHI
```

Con quest'ultima suddivisione, il rapporto fra $\frac{trB}{trT}$ raggiunge il risultato più alto fra i precedenti ed è poco inferiore a quello ottenuto con i due metodi precedenti e risulta essere un ottimo risultato dato il numero di cluster scelti.

La relativa suddivisione in cluster è la seguente:

```{r}
cutree(hlm, k=4, h=NULL)
```

$C_1$ *= {Emilia Romagna, Umbria, Valle d'Aosta, Veneto, Lombardia, Trentino-alto Adige, Friuli-venezia Giulia, Liguria, Marche, Piemonte, Abruzzo, Lazio, Toscana}*
$C_2$ *= {Molise}*
$C_3$ *= {Campania}*
$C_4$ *= {Sicilia, Sardegna, Basilicata, Puglia, Calabria}*

```{r}
plot(hlmedian, hang = -1, xlab = "Metodo gerarchico agglomerativo", 
     sub="della mediana")
axis(side = 4, at= round(c(0, hlmedian$height), 2))
rect.hclust(hlmedian, k=4)
```
