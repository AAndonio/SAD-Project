---
title: "SAD - Seconda parte"
author: "Antonio Vivone"
date: "01 marzo 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Distribuzione normale

La **funzione di distribuzione normale**, detta anche di Gauss o gaussiana, è molto importante nella statistica in quanto rappresenta una distribuzione limite alla quale tendono
varie altre funzioni di distribuzioni utilizzando opportune ipotesi.

Una variabile aleatoria X di densità di probabilità
$$f_X(x) = \frac{1}{\sigma \sqrt{2\pi}}exp \Big\{ -\frac{(x-\mu)^2}{2\sigma^2}\Big\}, \qquad x \in \mathbb{R} \qquad (\mu \in \mathbb{R}, \sigma>0)$$
si dice avere distribuzione normale di parametri $\mu$ e $\sigma$.

La densità normale:
* risulta essere simmetrica rispetto all'asse $x=\mu$ in quanto per ogni $x \in \mathbb{R}$ risulta che $f_X(\mu-x) = f_X(\mu+x)$;
* presenta il massimo $(\sigma\sqrt{2 \pi})^{-1}$ nel punto di ascissa $x=\mu$;
* presenta due flessi nei punti di ascisse $\mu - \sigma$ e $\mu + \sigma$.

Viene utilizzata la notazione $X \sim N(\mu, \sigma)$ per indicare che la variabile X ha distribuzione normale dei parametri $\mu$ e $\sigma$ ed è chiamata *variabile normale*.

In R utilizziamo la funzione:

```{r eval=FALSE}
dnorm(x, mean = mu, sd = sigma)
```
per calcolare la densità normale.

Quello che facciamo adesso è mostrare quello che accade quando i valori $\mu$ e $\sigma$ vengono modificati.
Procediamo quindi facendo variare $\mu$ e mantenendo $\sigma$ fissato.

```{r}
x <- seq(from = -7.5, to = 7.55, by = 0.1)

curve(dnorm(x, mean= 0, sd = 1), from = -6, to= 6, xlab = "x", ylab="f(x)", main = "mu = -2, -1, 0, 1, 2; sigma = 1", col="blue", lty=2)
curve(dnorm(x, mean= -1, sd = 1), from = -6, to= 6, xlab = "x", ylab="f(x)",add = TRUE, col="green")
curve(dnorm(x, mean= -2, sd = 1), from = -6, to= 6, xlab = "x", ylab="f(x)",add = TRUE, col="red")
curve(dnorm(x, mean= 1, sd = 1), from = -6, to= 6, xlab = "x", ylab="f(x)",add = TRUE, col="purple")
curve(dnorm(x, mean= 2, sd = 1), from = -6, to= 6, xlab = "x", ylab="f(x)",add = TRUE, col="orange")
```

La curva di colore blu tratteggiata rappresenta la curva con $\mu=0$. Facendo variare *mean*, la curva si sposta sull'asse delle ascisse mantenendo inalterato il suo valore.

Vediamo cosa succede modificando il valore *sd*.

```{r}
curve(dnorm(x, mean= 0, sd = 0.5), from = -6, to= 6, xlab = "x", ylab="f(x)", main = "mu = 0; sigma = 0.5, 1, 1.5", col="blue", lty=2)
curve(dnorm(x, mean= 0, sd = 1), from = -6, to= 6, xlab = "x", ylab="f(x)",add = TRUE, col="green")
curve(dnorm(x, mean= 0, sd = 1.5), from = -6, to= 6, xlab = "x", ylab="f(x)",add = TRUE, col="red")
```

Facendo variare il parametro $\sigma$ viene influenzata la larghezza della funzione: se infatti il parametro $\sigma$ cresce allora l'ordinata massima decresce e la curva diventa sempre più piatta; se invece $\sigma$ decresce allora l'ordinata massima cresce. Sono inversamente proporzionali in pratica.  
Notasi che l'area al di sotto continuerà sempre ad avere valore unitario.

### Funzione di distribuzione

La funzione di distribuzione di una variabile aleatoria $X \sim N(\mu, \sigma)$ è uguale a:

$$F_X(x) = P(X\leq x) = \int_{-\infty}^x f_X(y)dy = \Phi \big( \frac{x - \mu}{\sigma}\big) \qquad x \in \mathbb{R} $$

dove

$$\Phi(z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z} exp\big\{-\frac{y^2}{2}dy\big\}, \qquad z \in \mathbb{R}$$

è la funzione di distribuzione di una variabile aleatoria $Z\sim N(0,1)$ detta *normale standard*. Per questo, se $X \sim N(\mu,\sigma)$ si ha che:

$$P(a<X<b) = F_X(b) - F_X(a) = \phi \big( \frac{b-\mu}{\sigma}\big) - \phi \big( \frac{a-\mu}{\sigma}\big)$$
In R calcoliamo la funzione di distribuzione di una variabile $X \sim N(\mu,\sigma)$ tramite la funzione:
```{r eval=FALSE}
pnorm(x, mean=mu, sd=sigma, lower.tail=TRUE)
```

Come fatto in precedenza con la densità, procediamo al confronto fra le funzioni di distribuzioni ottenute facendo variare il parametro $\sigma$.

```{r}
curve(pnorm(x, mean = 0, sd = 0.5), from=-4, to=4, xlab = "x", ylab = expression(P(X<=x)), main="mu=0; sigma=0.5, 1, 1.5", lty=2)
text(-0.4, 0.8, "sigma=0.5")
curve(pnorm(x, mean=0, sd=1), add = TRUE, col="blue")
arrows(-1,0.1,0.3,0.2,code = 1,length = 0.10)
text(0.8, 0.2, "sigma=1")
curve(pnorm(x, mean=0, sd=1.5), add = TRUE, lty=3)
text(-2.2,0.2,"sigma=1.5")
```

### Regola del 3-$\sigma$

Per una qualsiasi variabile aleatoria normale $X \sim N(\mu, \sigma)$ risulta che:

$$P(\mu -3\sigma < X < \mu + 3\sigma) = P(-3 < \frac{X-\mu}{\sigma}< 3) = P(-3<Z<3) = 0.9973002$$

Quello che la regola vuol dire è che la probabilità che una variabile aleatoria $X \sim N(\mu, \sigma)$ assuma valori in un intervallo avente come centro $\mu$ e semiampiezza $3\sigma$ è mlto vicino all'unità, ovvero ad 1. Questa proprietà delle variabili aleatorie normali è detta **regola del 3$\sigma$**. 
Utilizzando la funzione *pnorm()* possiamo mostrare quanto detto:
```{r}
pnorm(3,mean = 0, sd = 1) - pnorm(-3, mean = 0, sd = 1)
```

### Quantili

È possibile anche calcolare i quantili (percentili) della distribuzione normale attraverso la funzione:
```{r eval=FALSE}
qnorm(z, mean = mu, sd = sigma, lower.tail = TRUE)
```
La funzione restituisce il percentile $z \cdot 100-esimo$, ovvero il più piccolo numero *x* assunto dalla variabile aleatoria normale X tale che $P(X\leq x) \geq z$.

Considerando ad esempio una variabile normale standard $Z \sim N(0, 1)$, è possibile ottenere i quartili nella seguente maniera:

```{r}
z<-c(0,0.25,0.5,0.75,1)
qnorm(z,mean=0, sd=1)
```

Fatto interessanta da notare è la simmetria fra $Q_1$ e $Q_3$ dovuto alla simmetria intorno all'origine della densità normale standard.

### Approssimare la distribuzione binomiale con la distribuzione normale

Siccome il calcolo delle probabilità binomiali risulta aumentare di complessità al crescere di *n*, sono state ricercate formule in grado di approssimare tale distribuzione con quella normale. Vediamo i due metodi proposti:

#### Teorema di De Moivre-Laplace

Sia $X_1, X_2,...$ una successione di variabili aleatorie indipendenti distribuite alla Bernoulli con parametro p ($0<p<1$), e sia $Y_n = X_1 + X_2+...+X_n$. Allora per ogni $x \in \mathbb{R}$ risulta che:

$$\lim\limits_{n \to \infty} P\bigg(\frac{Y_n-np}{\sqrt{np(1-p)}} \leq x\bigg) = \frac{1}{\sqrt2\pi}\int_{-\infty}^{x}e^{-y^2/2}dy$$
ovvero

$$\frac{Y_n - np}{\sqrt{np(1-p)}} \to Z$$
converge in distribuzione alla variabile aleatoria Z normale standard.

Se le variabili $X_1 + X_2+...$ sono variabili aleatorie di Bernoulli di parametro $p$, allora $Y_n = X_1 + X_2+...+X_n$ è una variabile aleatoria binomiale di valore medio $np$ e varianza $np(1-p)$. Quello che il teorema fa è mostrare che sottraendo a $Y_n$ la sua media $np$ e dividendo la differenza per la deviazione standard $\sqrt{np(1-p)}$, si ottiene una variabile aleatoria standardizzata la cui funzione di distribuzione è per $n$ grandi una normale standard approssimata.
L'approssimazione della binomiale alla normale è la seguente:

$$Y_n \simeq np + \sqrt{np(1-p)}Z$$

al variare di n con p fissato. La variabile aleatoria con densità normale ottenuta ha valore medio $np$ e varianza $np(1-p)$.

È possibile valutare l'approssimazione della binomiale ottenuta confrontandola con la densità normale di valore $np$ e varianza$np(1-p)$ per $n=25,50,75,100$ e $p=0.2$.

```{r}
par(mfrow=c(2,2))
p<-0.2
q<-1-p
x<-0:25
n<-25
curve(dnorm(x, n*p, sqrt(n*p*q)), from=n*p-3*sqrt(n*p*q), to=n*p+3*sqrt(n*p*q), xlab="x", ylab="P(X=x)", main="Binomiale, n=25, p=0.2")
lines(x, dbinom(x,n,0.2), type="h")
x<-0:50
n<-50
curve(dnorm(x, n*p, sqrt(n*p*q)), from=n*p-3*sqrt(n*p*q), to=n*p+3*sqrt(n*p*q), xlab="x", ylab="P(X=x)", main="Binomiale, n=50, p=0.2")
lines(x, dbinom(x,n,0.2), type="h")

x<-0:75
n<-75
curve(dnorm(x, n*p, sqrt(n*p*q)), from=n*p-3*sqrt(n*p*q), to=n*p+3*sqrt(n*p*q), xlab="x", ylab="P(X=x)", main="Binomiale, n=75, p=0.2")
lines(x, dbinom(x,n,0.2), type="h")

x<-0:100
n<-100
curve(dnorm(x, n*p, sqrt(n*p*q)), from=n*p-3*sqrt(n*p*q), to=n*p+3*sqrt(n*p*q), xlab="x", ylab="P(X=x)", main="Binomiale, n=100, p=0.2")
lines(x, dbinom(x,n,0.2), type="h")
```

Come è possibile vedere, l'approssimazione migliora quando p tende a $\frac{1}{2}$ e diventa eccellente quando p=$\frac{1}{2}$

#### Teorema centrale di convergenza

Il **teorema centrale di convergenza** fornisce un'approssimazione alla distribuzione della somma di variabili aleatorie indipendenti, evidenziando allo stesso tempo l'importanza della distribuzione normale.

Sia $X_1,X_2,...$ una successione di variabili aleatorie, definite nello stesso spazio di probabilità, indipendenti e identicamente distribuite con valore medio $\mu$ finito e varianza $\sigma^2$ finita e positiva. Posto per ogni intero n positivo $Y_n = X_1 + X_2 + ... + X_n$, per ogni $x\in \mathbb{R}$ risulta:

$$\lim\limits_{n \to \infty} P\bigg(\frac{Y_n-n\mu}{\sigma\sqrt{n}} \leq x\bigg) = \frac{1}{\sqrt2\pi}\int_{-\infty}^{x}e^{-y^2/2} \ dy = \Phi(x)$$
ovvero

$$\frac{Y_n-E(Y_n)}{\sqrt{Var(Y_n)}} = \frac{Y_n-n\mu}{\sigma\sqrt{n}} \to Z $$

Quello che il teorema mostra è che sottraendo a $Y_n$ la sua media e dividendo il tutto per la sua deviazione standard, ovvero $\sigma \sqrt{n}$ si ottiene una variabile aleatoria standardizzata la cui funzione di distribuzione per **n sufficientemente grandi** è approssimativamente una normale standard con valore $n\mu$ e varianza $n\sigma^2$. Quindi la bontà dell'approssimazione dipende dal tipo di distribuzione delle variabili $X_1, X_2, ..., X_n$ e dalle dimensioni di n. Spesso l'approssimazione risulta soddisfacente già con $n\geq 30$.

### Simulare una variabile in R

È possibile simulare in R una variabile aleatoria normale generando una sequenza di numeri pseudocasuali mediante la funzione:

```{r eval=FALSE}
rnorm(N, mean = mu, sd = sigma)
```

Confrontiamo quindi la densità normale teorica con la densità simulata. Vediamo la densità normale con $\mu = 2$ e $\sigma = 1$ con la densità simulata con $N=500, 5000, 50000$

```{r}
par ( mfrow = c (2 ,2) )
curve(dnorm(x,mean=2,sd =1),from=-2, to=6 , xlab="x", ylab="f(x)",
ylim = c (0 ,0.5) , main = " Densità normale con mu =2 , sigma =1 " )

sim1<-rnorm(500,mean=2,sd =1)
hist ( sim1 , freq =F , xlim = c ( -2 ,6) , ylim = c (0 ,0.5) , breaks =100 , xlab = " x " ,
ylab = " Istogramma " , main = " Densità simulata con N=500 " )

sim2 <- rnorm (5000 , mean =2 , sd =1)
hist ( sim2 , freq =F , xlim = c ( -2 ,6) , ylim = c (0 ,0.5) , breaks =100 , xlab = " x " ,
ylab = " Istogramma " , main = " Densità simulata con N=5000 " )

sim3 <- rnorm (50000 , mean =2 , sd =1)
hist ( sim3 , freq =F , xlim = c ( -2 ,6) , ylim = c (0 ,0.5) , breaks =100 , xlab = " x " ,
ylab = "Istogramma" , main = " Densità simulata con N=50000 " )
```

All'aumentare della numerosità, l'istogramma della densità simultata si avvicina sempre di più alla curva teorica. 

### Analisi di un campione normale

Procediamo adesso a generare un campione facente parte di una popolazione normale in modo tale da effettuare diverse analisi:

```{r }
campione <- rnorm(10000, mean = 2, sd = 1)
```

Riportiamo di seguito la media campionaria, la varianza campionaria e la deviazione standard campionaria del campione: 

```{r echo=FALSE}
mean(campione)
```

```{r echo=FALSE}
var(campione)
```

```{r echo=FALSE}
sd(campione)
```

Calcoliamo i quantili del campione e generiamo il boxplot in modo da capire come sono distribuiti i valori:

```{r}
quantile(campione)
```

```{r}
boxplot(campione, horizontal = T, main="Boxplot del campione normale", col="red")
```

Dal boxplot capiamo che il campione risulta simmetrico e centrato intorno al valore 2, come stabilito. Effettuiamo un'ultima analisi mostrando la densità tramite istogramma:

```{r}
hist ( campione , freq =F , xlim = c (-2,6) , ylim = c (0 ,0.5) , breaks =100 , xlab = " x " ,ylab = "Istogramma" , main = " Densità simulata con N=10000 " )
```

La forma della densità del campione è decisamente simile a quella della normale teorica, grazie soprattutto alla scelta del campione molto grande. 

Passiamo adesso alla stima dei parametri.

### Stima puntuale

#### Campioni casuali e stimatori

Uno dei problemi della inferenza statistica è quello di ottenere informazioni su parametri non noti di una popolazioni di cui si conosce però la forma della funzione di distribuzione.

Per ottenere informazioni sui parametri non noti è possibile fare uso dell'inferenza statistica e considerare un campione estratto dalla popolazione. Su questo campione poi utilizziamo alcune variabili aleatorie, ovvero funzioni misurabili del campione casuale, dette **statistiche** e **stimatori**. La definizione di stimatore è la seguente:

Uno stimatore **$\hat\theta = t(X_1, X_2,...,X_n)$** è una funzione misurabile e osservabile del campione casuale $(X_1, X_2,...,X_n)$ i cui valori vengono utilizzati per stimare un parametro non noto $\vartheta$ della popolazione. I valori $\hat\vartheta$ assunti da tale stimatore sono detti stime del parametro non noto $\vartheta$.

Stimatori tipici sono *media campionaria* e *varianza campionaria*.

*Proposizione:*
Sia $X_1,X_2,...,X_n$ un campione casuale estratto da una popolazione descritta da una variabile aleatoria ossevabile X caratterizzata da valore medio $E(X)=\mu$ finito e varianza $Var(X) = \sigma^2$ finita. Risulta:

$$E(\bar{X}) = \mu, \qquad  Var(\bar{X}) = \frac{\sigma^2}{n}$$
Per la proprietà di linearità del valore medio e l'identica distribuzione delle variabili aleatorie che costituiscono il campione della proposizione sopra descritta, si ha:

$$E(\bar{X}) = E\Big[\frac{1}{n} \sum_{i=1}^n X_i\Big] = \frac{1}{n} \sum_{i=1}^n E(X_i) = \mu $$

e per la varianza:

$$Var(\bar{X}) = Var\Big[\frac{1}{n} \sum_{i=1}^n X_i\Big] = \frac{1}{n^22} \sum_{i=1}^n Var(X_i) = \frac{\sigma^2}{n}$$

Questa proposizione ci dice che al crescere dell'ampiezza del campione, la media campionaria fornisce una stima sempre più accurata del valore medio della popolazione. Dal teorema centrale di convergenza sappiamo che per n sufficientemente grandi, la funzione di distribuzione della media campionaria $\bar{X}$ è approssimativamente normale con valore medio $\mu$ e varianza $\sigma^2 / n$.

#### Metodo per la ricerca di stimatori

Esistono due metodi maggiormente utilizzati per la ricerca di stimatori: il **metodo dei momenti** e il **metodo della massima verosomiglianza**.

##### Metodo dei momenti

Prima di parlare del metodo dei momenti è necessario introdurre i **momenti campionari**.

Si definisce **momento campionario r-esimo** relativo ai valori osservati (x_1, x_2,..., x_n) del campione casuale il valore

$$M_r(x_1, x_2,..., x_n) = \frac{1}{n} \sum_{i=1}^n x_i^r \qquad (r=1,2,...)$$

Il momento campionario r-esimo risulta essere quindi la media aritmetica delle potenze r-esime delle *n* osservazioni effettuate sulla popolazione. In particolare, se $r=1$ otteniamo la media campionaria $\bar{X}$.

Se esistono *k* parametri da stimare, il **metodo dei momenti** consiste nell'uguagliare i primi *k* momenti della popolazione in esame con quelli del campione casuale. Se i *k* momenti esistono e sono finiti il metodo consiste nel risolvere il sistema di equazioni:

$$E(X^r) = M_r(x_1, x_2,..., x_n) \qquad (r=1,2,...,k)$$

Le incognite sono i k parametri non noti $\vartheta_1, \vartheta_2,...,\vartheta_k$.
Per poter utilizzare questo metodo, il sistema deve ammettere un'unica soluzione. Le stime dipendono dal campione osservato e quindi, al variare del campione, cambiano anche queste ultime.
Gli stimatori prendono il nome di *stimatori del metodo dei momenti*.

Procediamo quindi ad utilizzare il metodo alla nostra analisi della popolazione normale. Siamo quindi interessati a fornire degli stimatori dei parametri $\mu$ e $\sigma^2$.
Siccome $E(X) = \mu$ e $E(X^2) = \sigma^2 + \mu^2$ si ha un sistema di due equazioni, in quanto due sono i parametri da stimare.
Si ha che:

$$\hat\mu = \frac{x_1+x_2+...+x_n}{n}; \qquad \hat\sigma^2 + \hat\mu^2 = \frac{(x_1 + x_2 +...+ x_n)^2}{n}$$

dalla seconda equazione ricaviamo: 

$$\hat\sigma^2 = \frac{(x_1 + x_2 +...+ x_n)^2}{n} - \frac{(x_1 + x_2 +...+ x_n)^2}{n^2} = \frac{1}{n} \Big[ \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^nx_i \Big] = \frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})^2 $$

Il metodo dei momenti ci fornisce quindi come stimatore del valore medio $/mu$ la media campionaria $\bar{X}$ e come stimatore della varianza $\sigma^2$ la variabile aleatoria $\frac{(n-1)}{n}S^2$.

Per quanto riguarda il nostro campione quindi abbiamo che:

```{r}
stimaMeanMomenti <- mean(campione)
stimaMeanMomenti
```

```{r}
stimaVarMomenti <- (length(campione)-1) * var(campione)/length(campione)
stimaVarMomenti
```

#### Metodo della massima verosomiglianza

Il **metodo della massima verosomiglianza** è il metodo più importante fra i due ed è anche preferito a quello dei momenti. Prima di illustrare il metodo va introdotta la **funzione di verosomiglianza**.

Sia $X_1, X_2, ..., X_n$ un campione casuale estratto dalla popolazione.
La **funzione di verosomiglianza** $L(\vartheta_1, \vartheta_2, ..., \vartheta_k) = L(\vartheta_1, \vartheta_2, ..., \vartheta_k; x_1, x_2,..., x_n)$ del campione osservato è la funzione di densità di probabilità congiunta del campione casuale $X_1, X_2,..., X_n$ ovvero:

$$L(\vartheta_1, \vartheta_2, ..., \vartheta_k) = L(\vartheta_1, \vartheta_2, ..., \vartheta_k; x_1, x_2,..., x_n) = f(x_1:\vartheta_1, \vartheta_2, ..., \vartheta_k)f(x_2:\vartheta_1, \vartheta_2, ..., \vartheta_k)\quad...\quad f(x_n:\vartheta_1, \vartheta_2, ..., \vartheta_k) $$

Il metodo consiste quindi nel massimizzare la funzione di verosomiglianza rispetto ai parametri $\vartheta_1, \vartheta_2, ..., \vartheta_k$ cercando di determinare da quale funzione di densità di probabilità congiunta è **più verosimile** che provenga il campione osservato. Si cercando quindi i valori $\vartheta_1, \vartheta_2, ..., \vartheta_k$ che massimizzino la funzione di verosomiglianza e, una volta trovati, vengono indicati nella seguente maniera $\hat\vartheta_1, \hat\vartheta_2, ..., \hat\vartheta_k$ e costituiscono le **stime di massima verosomiglianza**. Siccome queste stime dipendono dal campione, facendo variare il campione osservato si ottengono gli stimatori di massima verosomiglianza $\bar\Theta_1, \bar\Theta_2, ..., \bar\Theta_k$ dei parametri non noti $\vartheta_1, \vartheta_2, ..., \vartheta_k$ della popolazione, detti **stimatori di massima verosomiglianza**.

Per quanto riguarda la popolazione normale da noi presa in esame abbiamo:

$$\hat\mu = \bar{x}, \qquad \hat\sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2$$
Quindi lo stimatore di massima verosomiglianza di $\mu$ risulta essere la media campionaria di $\bar{X}$, mentre lo stimatore di $\sigma^2$ risulta essere pari a $\frac{(n-1)}{n}S^2$.
Gli stimatori ottenuti coincidono con quelli ottenuti con il metodo dei momenti, quindi eviteremo di ripetere i calcoli.

### Proprietà degli stimatori

Data la presenza di diversi stimatori in grado di stimare un parametro non noto di una popolazione occorre definire alcune proprietà che li caratterizzano.
Queste proprietà sono:

* corretto,
* più efficiente di un altro,
* corretto e con varianza uniformamente minima,
* asintoticamente corretto
* consistente.

Uno simatore è **corretto** se il suo valore medio è uguale al corrispondente parametro non nodo della popolazione. È possibile avere più stimatori corretti per stimare un parametro. Per scegliere quale utilizzare, si vede quale risulta essere più efficiente confrontando la varianza degli stimatori e scegliendo quello con varianza più piccola. Altro metodo è quello della ricerca dello stimatore con **errore quadratico uniformemente minimo** per la classe degli stimatori corretti. 

Come visto prima, sia dal metodo dei momenti che da quello della massima verosomiglianza sono stati ricavati gli stessi stimatori per i parametri $\mu$ e $\sigma^2$. Per $\mu$ lo stimatore ricavato è la media campionaria, che risulta essere **corretto con varianza minima** e **consistente**, mentre per $\sigma^2$ abbiamo $\frac{n-1}{n} S^2$ che è **asintoticmanete corretto** e **consistente**.
\underline{C}_n$ e $\bar{C}_n
## Stima intervallare

Spesso si preferisce sostituire alla stima puntale di un parametro non noto un intervallo di valori, chiamato **intervallo di confidenza**, entro il quale sia compreso il valore del parametro non noto con un certo **grado di fiducia** o **coefficiente di confidenza**. Supponiamo di avere un campione casuale con una densità di probabilità o funzione di probabilità uguale a $f(x;\vartheta)$ dove $\vartheta$ rappresenta il parametro non noto della popolazione. Per definire questo intervallo, ci occorre denotare due statistiche, $\underline{C}_n$ e $\bar{C}_n$ che soddisfino la seguente condizione $\underline{C}_n<\bar{C}_n$. 
Fissato un coefficiente di confidenza $1-\alpha$ ($0<\alpha<1$), se è possibile scegliere le statistiche $\underline{C}_n$ e $\bar{C}_n$ in modo tale che

$$P(\underline{C}_n < \vartheta <\bar{C}_n) = 1 - \alpha$$
 allora si dice che $(\underline{C}_n,\bar{C}_n)$ è un **intervallo di confidenza** di grado $1-\alpha$ per $\vartheta$. Le statistiche $\underline{C}_n$ e $\bar{C}_n$ sono dette rispettivamente *limite inferiore* e *limite superiore*.
La stima puntale detta in precedenza deve ricadere in questo intervallo.

Vediamo adesso i metodi per la costruzione di questo intervallo di confidenza.

### Metodo pivotale

Il **metodo pivotale** è uno dei metodi a disposizione per la costruzione degli intervalli di confidenza. Esso consiste nel determinare una variabile aleatoria $\gamma(X_1, X_2, ..., X_n;\vartheta)$ che dipende dal campione casuale e dal parametro non noto $\vartheta$ la cui **funzione di distribuzione non contiene il parametro da stimare**. Questa variabile aleatoria non è statistica in quanto dipende dal parametro non noto ed è quindi non osservabile.

Per ogni fissato coefficiente $\alpha$ ($0<\alpha<1$) siano $\alpha_1$ e $\alpha_2$ ($\alpha_1 <\alpha_2$) due valori dipendenti soltanto dal coefficiente fissato $\alpha$ tali che per ogni $\vartheta \in \Theta$ si abbia:

$$P(\alpha_1 < \gamma(X_1, X_2, ..., X_n;\vartheta) < \alpha_2) = 1 - \alpha$$

Se per ogni campione osservato $(x_1, x_2,..., x_n)$ e per ogni parametro non noto $\vartheta \in \Theta$ si riesce a dimostrare che:

$$\alpha_1 < \gamma(x:\vartheta) < \alpha_2 \iff g_1(x) < \vartheta < g_2(x)$$

con $g_1$ e $g_2$ dipendenti dal campione osservato allora è possibile riscrivere la probabilità sopra descritta come:

$$P(g_1(X_1, X_2, ..., X_n)< \vartheta < g_2(X_1, X_2, ..., X_n)) = 1 - \alpha$$

denotando con $\underline{C}_n = g_1(X_1, X_2, ..., X_n)$ e $\bar{C}_n = g_2(X_1, X_2, ..., X_n)$ deduciamo che $(\underline{C}_n,\bar{C}_n)$ è un intervallo di confidenza di grado $1-\alpha$ per $\vartheta$.

Per una popolazione normale è possibile analizzare i seguenti problemi:

1. Determinare un intervallo di confidenza di grado 1 - $\alpha$ per il valore medio $\mu$ nel caso in cui la varianza $\sigma^2$ della popolazione normale è nota.

**Intervallo di confidenza per $\mu$ con $\sigma^2$ nota**

Per determinare un intervallo di confidenza $\alpha - 1$ per il valore medio $\mu$ nel caso in cui la varianza $\sigma^2$ della popolazione normale è nota, utilizziamo il metodo pivotale visto primo e consideriamo la variabile aleatoria standardizzata di valore medio nullo e varianza unitaria

$$Z_n = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}}$$

Questa variabile aleatoria risulta essere una normale standard che dipende dal campione casuale e dal parametro non noto $\mu$ e quindi è possibile utilizzarla come variabile di pivot.
Scegliendo nel metodo pivotale $\alpha_1 = -z_{a/2}$ e $\alpha_2 = z_{a/2}$ è tale che

$$P(Z_n < -z_{a/2}) = P(Z_n > z_{a/2}) = \frac{a}{2}$$

si ha 

$$P(-z_{a/2} < Z_n < z_{a/2}) = 1 - \alpha$$

Graficamente abbiamo:

```{r echo=FALSE}
curve(dnorm (x,mean=0,sd=1) ,from=-3, to=3, axes=FALSE ,ylim=c(0 ,0.5) ,
xlab="",ylab="",main="Densità normale standard")
text (0,0.05, expression (1- alpha))
axis(1,c(-3,-1,0,1,3) ,c("",expression (-z[alpha /2]) ,
0, expression (z[alpha /2]) ,""))
vals <-seq(-3,-1, length =100)
x<-c(-3,vals ,-1,-3)
y<-c(0, dnorm(vals) ,0,0)
polygon (x,y,density =20, angle =45)
vals <-seq(1,3, length =100)
x<-c(1,vals ,3,1)
y<-c(0, dnorm(vals) ,0,0)
polygon (x,y,density =20, angle =45)
abline (h=0)
text (-1.5,0.05, expression (alpha/2))
text (1.5 ,0.05 , expression (alpha/2))
box ()
```

L'intervallo di confidenza con grado di fiducia $1-\alpha$ per il valore medio $\mu$ è pari a

$$\bar{x}_n - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu < \bar{x}_n + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$$
Per quanto riguarda la nostra analisi, vogliamo ottenere un grado di fiducia pari a $0.95$ quindi poniamo $\alpha=0.05$ e supponiamo che la varianza

```{r}
alpha <- 1-0.95

n<-length(campione)

#limite inferiore
mean(campione)-qnorm(1-alpha/2, mean=0, sd=1)*8/sqrt(n)
```

```{r}
#limite superiore
mean(campione)+qnorm(1-alpha/2, mean=0, sd=1)*8/sqrt(n)
```

2. Determinare un intervallo di confidenza di grado $1-\alpha$ per il valore medio $\mu$ nel caso in cui la varianza della popolazione normale è non nota

**Intervallo di confidenza per $\mu$ con varianza $\sigma^2$ non nota**

Per determinare un intervallo di confidenza in questo caso, quello che facciamo è utilizzare il metodo pivotale e considerare la variabile aleatoria di pivot pari a:

$$T_n = \frac{\bar{X_n} - \mu}{S_n/\sqrt{n}}$$

Questa variabile dipende dal campione casuale e dal parametro non noto $\mu$ e può essere quindi interpretata come una variabile aleatoria di pivot. La variabile in questione risulta anche essere distribuita con la legge di Student con $n-1$ gradi di libertà.
Scegliendo nel metodo pivotale $\alpha_1 = -t_{\alpha/2,n-1}$ e $\alpha_2=t_{\alpha/2,n-1}$ dove $t_{\alpha/2,n-1}$ abbiamo che:

$$P(-t_{\alpha/2,n-1} < T_n < t_{\alpha/2,n-1}) = 1 - \alpha$$
abbiamo:
```{r echo=FALSE}
curve(dt(x,df =5) ,from=-3, to=3, axes=FALSE ,ylim=c(0 ,0.5) ,xlab="",
ylab="",main="Densità di Student con n-1 gradi di libertà")
text(0,0.05, expression (1- alpha ))
axis(1,c(-3,-1,0,1,3) ,c("",expression (-t[list(alpha/2,n-1) ]) ,0,
expression (t[list(alpha/2,n-1) ]),""))
vals <-seq(-3,-1, length =100)
x<-c(-3,vals ,-1,-3)
y<-c(0,dt(vals ,df=5) ,0,0)
polygon (x,y,density =20, angle =45)
vals <-seq(1,3, length =100)
x<-c(1,vals ,3,1)
y<-c(0,dt(vals ,df=5) ,0,0)
polygon (x,y,density =20, angle =45)
abline (h=0)
text (-1.5,0.05, expression (alpha/2))
text (1.5 ,0.05 , expression (alpha/2))
box ()
```

Una stima dell'intervallo di confidenza $1-\alpha$ per il valore medio $\mu$ risulta essere:

$$\bar{x_n}-t_{\alpha/2,n-1}\frac{s_n}{\sqrt{n}} < \mu < \bar{x_n}+t_{\alpha/2,n-1}\frac{s_n}{\sqrt{n}}$$

Ponendo come nel caso precedente vogliamo ottenere un grado di fiducia pari a 95 quindi poniamo $\alpha = 0.05$ e andiamo a stimare l'intervallo di confidenza:

```{r}
alpha <- 1-0.95

ds <- sd(campione)
n <- length(campione)

#stima limite inferiore
mean(campione)-qt(1-alpha/2, df=n-1)*ds/sqrt(n)
```

```{r}
#stima limite superiore
mean(campione)+qt(1-alpha/2, df=n-1)*ds/sqrt(n)
```

3. Determinare un intervallo di confidenza di grado $1-\alpha$ per la varianza nota $\sigma^2$ nel caso in cui il valore medio della popolazione normale $\mu$ risulti essere noto.

**Intervallo di confidenza per $\sigma^2$ con \mu noto**

Come nei casi precedenti, anche qui utilizziamo il metodo pivotale e consideriamo come variabile di pivot la seguente:

$$V_n = \sum_{i=1}^n \bigg(\frac{X_i-\mu}{\sigma}\bigg)^2 = \frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i-\mu)^2$$

Questa variabile dipende dal campione casuale e dal parametro non noto $\sigma^2$ ed è distribuita con legge chi-quadrato con *n* gradi di libertà, essendo costituita dalla somma dei quadrati di *n* variabili aleatorie normali standard.
Nel metodo pivotale, scegliamo $\alpha_1=\chi_{1-\alpha/2,n}^{2}$ e $\alpha_2=\chi_{\alpha/2,n}^{2}$ avendo così:

$$P(\chi_{1-\alpha/2,n}^{2} < V_n < \chi_{\alpha/2,n}^{2}) = 1-\alpha$$
graficamente:

```{r echo=FALSE}
curve(dchisq (x,df=6) ,from=0, to=12, axes=FALSE ,ylim=c(0 ,0.15),
xlab="",ylab="",main="Densità chi-quadrato con n gradi di libertà")
text (4,0.02, expression (1- alpha))
axis(1,c(0,2,4,6,12) ,c("",expression ({ chi ^2}[ list(1- alpha/2,n)]),expression (n-2) ,expression ({chi ^2}[ list(alpha /2,n)]),""))
vals <-seq(0,2, length =100)
x<-c(0,vals ,2,0)
y<-c(0, dchisq (vals ,df=6) ,0,0)
polygon (x,y,density =20, angle =45)
vals <-seq (6,12, length =100)
x<-c(6,vals ,12 ,6)
y<-c(0, dchisq (vals ,df=6) ,0,0)
polygon (x,y,density =20, angle =45)
abline (h=0)
text (1.2 ,0.02 , expression (alpha /2))
text (8.5 ,0.02 , expression (alpha /2))
box ()
```

Una stima dell'intervallo di confidenza $1-\alpha$ per $\sigma^2$ è:

$$\frac{(n-1)s_n^2+ n(\overline{X}_n - \mu)^2}{\chi^2_{\alpha/2,n}} < \sigma^2 < \frac{(n-1)s_n^2+ n(\overline{X}_n - \mu)^2}{\chi^2_{1-\alpha/2,n}}$$

Il grado di fiducia che vogliamo ottenere è pari a 95, quindi poniamo $\alpha=0.05$ e supponiamo che la media nota sia $\mu=2$, stimiamo adesso il parametro:

```{r}
alpha <- 1-0.95
n <-length(campione)
mu <- 2

#limite inferiore
((n-1)*var (campione)+n*(mean(campione)-mu)**2)/qchisq(1- alpha/2,df=n)
```

```{r}
#limite superiore
((n-1)*var (campione)+n*(mean(campione)-mu)**2)/qchisq(alpha/2,df=n)
```

4. Determinare un intervallo di confidenza di grado $1-\alpha$ per la varianza nota $\sigma^2$ nel caso in cui il valore medio della popolazione normale non è noto.

**Intervallo di confidenza per $\sigma^2$ con valore medio $\mu$ non noto**

Siccome qui non abbiamo a disposizione la media, utilizziamo la media campionaria. La variabile aleatoria di pivot utilizzata è la seguente:

$$Q_n = \frac{(n-1)S_n^2}{\sigma} = \frac{1}{\sigma^2}\sum_{i=1}{n}(X_i-\bar{X_n})^2$$

Questa variabile dipende dal campione casuale e dal parametro non noto $\sigma^2$ ed è distribuita con legge chi-quadrato con n-1 gradi di libertà.
Poniamo $\alpha_1=\chi_{1-\alpha/2,n}^{2}$ e $\alpha_2=\chi_{\alpha/2,n}^{2}$ e abbiamo che:

$$P(\chi_{1-\alpha/2,n}^{2} < Q_n < \chi_{\alpha/2,n}^{2}) = 1-\alpha$$
graficamente:

```{r echo=FALSE}
curve(dchisq (x,df=6) ,from=0, to=12, axes=FALSE ,ylim=c(0 ,0.15),
xlab="",ylab="",main="Densità chi-quadrato con n-1 gradi di libertà")
text(4,0.02, expression (1- alpha ))
axis(1,c(0,2,4,6,12) ,c("",expression ({chi ^2}[ list(1- alpha/2,n-1) ]),expression (n-2) ,expression ({chi ^2}[ list(alpha /2,n-1) ]),""))
vals <-seq(0,2, length =100)
x<-c(0,vals ,2,0)
y<-c(0, dchisq (vals ,df=6) ,0,0)
polygon (x,y,density =20, angle =45)
vals <-seq (6,12, length =100)
x<-c(6,vals ,12 ,6)
y<-c(0, dchisq(vals ,df =6) ,0,0)
polygon (x,y,density =20, angle =45)
abline (h=0)
text (1.2 ,0.02 , expression (alpha/2))
text (8.5 ,0.02 , expression (alpha/2))
box ()
```

Una stima dell'intervallo di confidenza per $1-\alpha$ per $\sigma^2$ è:

$$\frac{(n-1)s_n^2}{\chi_{1-\alpha/2,n}^{2}} < \sigma^2 < \frac{(n-1)s_n^2}{\chi_{\alpha/2,n}^{2}}$$

Il grado di fiducia che vogliamo ottenere è pari a 0.95 quindi poniamo $\alpha=0.05$ e stimiamo così $\sigma^2$:

```{r}
alpha <- 1-0.95
n <- length(campione)

#stima del limite inferiore
(n-1)*var(campione)/qchisq (1- alpha/2,df=n-1)
```

```{r}
#stima del limite superiore
(n-1)*var(campione)/qchisq (alpha/2,df=n-1)
```


In conclusione sulla stima, per la popolazione normale le stime per intervallo del valore medio $\mu$ e della varianza $\sigma^2$  possono essere effettuate **qualsiasi sia la dimensione del campione casuale osservato**, questo grazie al fatto che conosciamo la distribuzione esatta della variabile pivotale usata: normale, di Student e chi-quadrato.

I casi interessanti che rappresentano davvero situazioni reali sono il secondo e il quarto, dove nel secondo ricordiamo che vogliamo stimare il valore medio $\mu$ con varianza $\sigma^2$ non nota e nel quarto dove vogliamo stimare la varianza $\sigma^2$ con $\mu$ non noto.

### Differenza tra valori medi

Diverse problematiche richiedono di confrontare i valori medi di due popolazioni quindi vediamo come determinare gli intervalli di confidenza per la differenza tra i valori medi di due popolazioni normali.

Introduciamo un nuovo campione per il confronto

```{r}
campione2 <- rnorm(11000, mean = 50, sd = 1)
```

Riportiamo di seguito la media campionaria, la varianza campionaria e la deviazione standard campionaria del campione: 

```{r echo=FALSE}
mean(campione2)
```

```{r echo=FALSE}
var(campione2)
```

```{r echo=FALSE}
sd(campione2)
```

I quantili:

```{r echo=FALSE}
quantile(campione2)
```

Adesso che abbiamo due campioni casuali indipendenti $X_1, X_2,..., X_{n_1}$ e $Y_1, Y_2,..., Y_{n_2}$ di ampiezza $n_1$ e $n_2$ estratti da due popolazioni normali $N(mu_1,\sigma_1^2)$ e $N(mu_2,\sigma_2^2)$, vogliamo analizzare i seguenti problemi:

1. Determinare un intervallo di confidenza di grado $1-\alpha$ per $\mu_1 - \mu_2$ quando entrambe le varianze $\sigma_1^2$ e $\sigma_2^2$ sono note;

Denotiamo con $\bar{X_{n_1}}$ e $\bar{X_{n_2}}$ le medie campionarie delle due popolazioni normali. Per ipotesi i campioni risultano essere **indipendenti** e la statistica $\bar X_{n_1}$ e $\bar Y_{n_2}$ risulta essere distribuita normalmente con valore medio $\mu_1 - \mu_2$ e varianza $\sigma_1^2/n_1 + \sigma_2^2/n_2$.

Per determinare un intervallo di confidenza di grado $1-\alpha$ con le varianze note consideriamo la seguente variabile aleatoria di pivot:

$$Z_n = \frac{\overline X_{n_1} - \overline Y_{n_2} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}$$

la variabile appena descritta dipende dal campione casuale e dal parametro non noto $\mu_1 - \mu_2$ ed è caratterizzata da una *densità normale standard*.

L'intervallo ricavato per una stima dell'intervallo di confidenza $1-\alpha$ per la differenza $\mu_1-\mu2$ è:

$$\overline{X}_{n1} - \overline{Y}_{n2} - z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}} < \mu_1 - \mu_2 <\overline{X}_{n1} - \overline{Y}_{n2} + z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}$$